{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "307b3ebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>@import url('https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap');\n",
       "@import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;700&display=swap');\n",
       "@import url('https://fonts.googleapis.com/css2?family=Source+Sans+3:wght@400;700&display=swap');\n",
       "@import url('https://fonts.googleapis.com/css2?family=Merriweather:wght@400;700&display=swap');\n",
       "\n",
       "\n",
       "/* Text areas (Classic Notebook + JupyterLab) */\n",
       ".rendered_html, .text_cell_render, .jp-RenderedHTMLCommon,\n",
       "div.rendered_html, .output_subarea {\n",
       "  font-family: \"Roboto\", \"Inter\", \"Source Sans 3\", system-ui, -apple-system, \"Segoe UI\", Arial, sans-serif; !important;\n",
       "}\n",
       "\n",
       "/* RTL for rendered markdown/html content */\n",
       ".rendered_html, .text_cell_render, .jp-RenderedHTMLCommon,\n",
       "h1, h2, h3, h4, h5, h6, p, strong, label, ol, ul, li {\n",
       "  line-height: 1.9;\n",
       "  font-family: \"Roboto\", \"Inter\", \"Source Sans 3\", system-ui, -apple-system, \"Segoe UI\", Arial, sans-serif; !important;\n",
       "}\n",
       "\n",
       "h1 { font-size: 2rem; color: #070F2B; font-weight: 900}\n",
       "h2 { font-size: 1.8rem; color: #070F2B; font-weight: 800}\n",
       "h3 { font-size: 1.6rem; color: #070F2B; font-weight: 700}\n",
       "h4 { font-size: 1.4rem; color: #1E3E62; font-weight: 600}\n",
       "h5 { font-size: 1.2rem; color: #1E3E62}\n",
       "h6 { font-size: 1rem; color: #1E3E62}\n",
       "p { font-size: 1.1rem; ; font-weight: 400}\n",
       "ol, ul, li, th, td  { font-size: 1rem; }\n",
       "\n",
       "/* Keep code + outputs LTR and monospace */\n",
       ".CodeMirror, .CodeMirror *,\n",
       "pre, code,\n",
       ".jp-CodeCell .jp-InputArea, .jp-CodeCell .jp-InputArea *,\n",
       ".jp-OutputArea, .jp-OutputArea * {\n",
       "  direction: ltr !important;\n",
       "  text-align: left !important;\n",
       "  font-size: 1rem !important;\n",
       "  font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, \"Liberation Mono\", monospace !important;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from IPython.display import HTML, display\n",
    "css = Path(\"../../../css/custom.css\").read_text(encoding=\"utf-8\")\n",
    "display(HTML(f\"<style>{css}</style>\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bd5570",
   "metadata": {},
   "source": [
    "# Chapter 2 — Basics of Data and Preprocessing\n",
    "## Lesson 12: Train/Validation/Test Hygiene (Temporal Splits, Group Splits, Entity Leakage)\n",
    "\n",
    "This lesson is about making evaluation realistic. The most sophisticated model is not useful if the evaluation overstates real-world performance.\n",
    "\n",
    "You will work through time-aware splitting, group/entity splitting, leakage forensics, and safe preprocessing patterns. The code is designed for tabular ML workflows common in industry.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400b4eaa",
   "metadata": {},
   "source": [
    "### Learning objectives\n",
    "\n",
    "By the end of this lesson you should be able to:\n",
    "\n",
    "1. Explain why evaluation requires (approximate) independence between training and testing.\n",
    "2. Decide when random splits are acceptable and when they are misleading.\n",
    "3. Implement temporal splits, rolling/expanding validation, and time gaps.\n",
    "4. Implement group/entity splits and group-aware cross-validation.\n",
    "5. Detect entity overlap, near-duplicates, and post-outcome features.\n",
    "6. Prevent preprocessing leakage by construction using pipelines.\n",
    "7. Separate model selection from final testing (avoid validation leakage).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f478f265",
   "metadata": {},
   "source": [
    "### Core idea: what your test set is estimating\n",
    "\n",
    "Let the deployment distribution be $\\mathcal{P}$ and the loss be $\\ell(\\cdot)$. The generalization risk is:\n",
    "\n",
    "$$R(f) = \\mathbb{E}_{(X,Y) \\sim \\mathcal{P}}[\\ell(f(X), Y)].$$\n",
    "\n",
    "A test set is useful only if it approximates an i.i.d. sample from $\\mathcal{P}$ *at the time and granularity of deployment*.\n",
    "When the split violates that approximation (because of time dependence, repeated entities, or leakage), the test estimate becomes optimistic.\n",
    "\n",
    "A compact way to see the optimism is through dependence. Suppose $\\mathcal{D}_{\\text{train}}$ and $\\mathcal{D}_{\\text{test}}$ are not independent. Then the expected test risk is generally:\n",
    "\n",
    "$$\\mathbb{E}[\\widehat{R}_{\\text{test}}(f)] \\ne R(f),$$\n",
    "\n",
    "because the training procedure may exploit information correlated across the two sets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f00a61f",
   "metadata": {},
   "source": [
    "### Leakage taxonomy\n",
    "\n",
    "Leakage is any path by which information unavailable at prediction time influences training or evaluation.\n",
    "\n",
    "- **Temporal leakage:** learning from the future (directly or via statistics that include future rows).\n",
    "- **Group/entity leakage:** the same entity appears in train and test; the model captures entity-specific signals.\n",
    "- **Target leakage:** a feature is a direct or indirect proxy for the label (post-outcome).\n",
    "- **Validation leakage:** hyperparameters or feature choices are tuned by repeatedly checking the test set.\n",
    "\n",
    "Operational rule: define the prediction timestamp/event, then remove any feature that would be unknown at that moment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c3c0c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, TimeSeriesSplit, GroupShuffleSplit, GroupKFold, KFold\n",
    ")\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, r2_score, mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pd.set_option('display.max_columns', 60)\n",
    "pd.set_option('display.width', 160)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6524a2",
   "metadata": {},
   "source": [
    "## 1) A clean evaluation protocol\n",
    "\n",
    "A robust protocol for most tabular ML projects:\n",
    "\n",
    "1. **Specify the prediction contract**: what exactly is predicted, when, and with what inputs.\n",
    "2. **Design the split** to match deployment (random, temporal, group, or a combination).\n",
    "3. **Lock the test set**: do not use it for iterative decisions.\n",
    "4. **Use validation/CV on training data** for model selection.\n",
    "5. **Report uncertainty** (fold variability) when possible.\n",
    "\n",
    "Three-way splitting can be described as:\n",
    "\n",
    "$$\\mathcal{D} = \\mathcal{D}_{\\text{train}} \\cup \\mathcal{D}_{\\text{val}} \\cup \\mathcal{D}_{\\text{test}},\\quad\n",
    "\\mathcal{D}_{\\text{train}} \\cap \\mathcal{D}_{\\text{val}} = \\varnothing,\\quad\n",
    "\\mathcal{D}_{\\text{train}} \\cap \\mathcal{D}_{\\text{test}} = \\varnothing,\\quad\n",
    "\\mathcal{D}_{\\text{val}} \\cap \\mathcal{D}_{\\text{test}} = \\varnothing.$$\n",
    "\n",
    "The key is that “disjoint” must also hold along dependency structure: time must not run backward, and entities must not overlap when the goal is generalization to new entities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2f3cf4",
   "metadata": {},
   "source": [
    "### When is a random split acceptable?\n",
    "\n",
    "A random split is often acceptable when:\n",
    "\n",
    "- Rows are approximately i.i.d. (no meaningful time ordering, no repeated entities).\n",
    "- The deployment distribution is stable (no major drift).\n",
    "- The model will be used on “similar” data to what was collected.\n",
    "\n",
    "If any of the following is true, random splitting becomes risky:\n",
    "\n",
    "- Multiple rows per entity.\n",
    "- Time dependence or seasonality.\n",
    "- Operational changes (policy shifts, product changes, sensor upgrades).\n",
    "- Features that are aggregates over time windows.\n",
    "\n",
    "In such cases, a time-aware or entity-aware split is typically required.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b49425",
   "metadata": {},
   "source": [
    "## 2) Temporal splits\n",
    "\n",
    "Use temporal splits when the task is forward-looking or the data-generating process changes over time.\n",
    "\n",
    "A time-respecting holdout is:\n",
    "\n",
    "$$\\text{Train} = \\{t \\le t_0\\},\\quad \\text{Test} = \\{t > t_0\\}.$$\n",
    "\n",
    "If labels are observed with delay or features use time windows, also consider a **gap** to prevent subtle look-ahead.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35dcbdd",
   "metadata": {},
   "source": [
    "### Example A: Random split vs time split on consumer complaints\n",
    "\n",
    "Dataset: `ConsumerComplaints.csv` with `Date Received`.\n",
    "\n",
    "Task: predict whether a complaint got a timely response (`Timely Response`).\n",
    "We compare random stratified splitting with a forward-in-time split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b55b3be4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date Received</th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Sub Product</th>\n",
       "      <th>Issue</th>\n",
       "      <th>Sub Issue</th>\n",
       "      <th>Consumer Complaint Narrative</th>\n",
       "      <th>Company Public Response</th>\n",
       "      <th>Company</th>\n",
       "      <th>State Name</th>\n",
       "      <th>Zip Code</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Consumer Consent Provided</th>\n",
       "      <th>Submitted via</th>\n",
       "      <th>Date Sent to Company</th>\n",
       "      <th>Company Response to Consumer</th>\n",
       "      <th>Timely Response</th>\n",
       "      <th>Consumer Disputed</th>\n",
       "      <th>Complaint ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-07-29</td>\n",
       "      <td>Consumer Loan</td>\n",
       "      <td>Vehicle loan</td>\n",
       "      <td>Managing the loan or lease</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wells Fargo &amp; Company</td>\n",
       "      <td>VA</td>\n",
       "      <td>24540</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Phone</td>\n",
       "      <td>2013-07-30</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>468882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-07-29</td>\n",
       "      <td>Bank account or service</td>\n",
       "      <td>Checking account</td>\n",
       "      <td>Using a debit or ATM card</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wells Fargo &amp; Company</td>\n",
       "      <td>CA</td>\n",
       "      <td>95992</td>\n",
       "      <td>Older American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Web</td>\n",
       "      <td>2013-07-31</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>468889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-07-29</td>\n",
       "      <td>Bank account or service</td>\n",
       "      <td>Checking account</td>\n",
       "      <td>Account opening, closing, or management</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Santander Bank US</td>\n",
       "      <td>NY</td>\n",
       "      <td>10065</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fax</td>\n",
       "      <td>2013-07-31</td>\n",
       "      <td>Closed</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>468879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-07-29</td>\n",
       "      <td>Bank account or service</td>\n",
       "      <td>Checking account</td>\n",
       "      <td>Deposits and withdrawals</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wells Fargo &amp; Company</td>\n",
       "      <td>GA</td>\n",
       "      <td>30084</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Web</td>\n",
       "      <td>2013-07-30</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>468949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-07-29</td>\n",
       "      <td>Mortgage</td>\n",
       "      <td>Conventional fixed mortgage</td>\n",
       "      <td>Loan servicing, payments, escrow account</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Franklin Credit Management</td>\n",
       "      <td>CT</td>\n",
       "      <td>6106</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Web</td>\n",
       "      <td>2013-07-30</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>475823</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Date Received             Product Name                  Sub Product                                     Issue Sub Issue Consumer Complaint Narrative  \\\n",
       "0    2013-07-29            Consumer Loan                 Vehicle loan                Managing the loan or lease       NaN                          NaN   \n",
       "1    2013-07-29  Bank account or service             Checking account                 Using a debit or ATM card       NaN                          NaN   \n",
       "2    2013-07-29  Bank account or service             Checking account   Account opening, closing, or management       NaN                          NaN   \n",
       "3    2013-07-29  Bank account or service             Checking account                  Deposits and withdrawals       NaN                          NaN   \n",
       "4    2013-07-29                 Mortgage  Conventional fixed mortgage  Loan servicing, payments, escrow account       NaN                          NaN   \n",
       "\n",
       "  Company Public Response                     Company State Name Zip Code            Tags Consumer Consent Provided Submitted via Date Sent to Company  \\\n",
       "0                     NaN       Wells Fargo & Company         VA    24540             NaN                       NaN         Phone           2013-07-30   \n",
       "1                     NaN       Wells Fargo & Company         CA    95992  Older American                       NaN           Web           2013-07-31   \n",
       "2                     NaN           Santander Bank US         NY    10065             NaN                       NaN           Fax           2013-07-31   \n",
       "3                     NaN       Wells Fargo & Company         GA    30084             NaN                       NaN           Web           2013-07-30   \n",
       "4                     NaN  Franklin Credit Management         CT     6106             NaN                       NaN           Web           2013-07-30   \n",
       "\n",
       "  Company Response to Consumer Timely Response Consumer Disputed  Complaint ID  \n",
       "0      Closed with explanation             Yes                No        468882  \n",
       "1      Closed with explanation             Yes                No        468889  \n",
       "2                       Closed             Yes                No        468879  \n",
       "3      Closed with explanation             Yes                No        468949  \n",
       "4      Closed with explanation             Yes                No        475823  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complaints_path = \"../../../Datasets/Clustering/ConsumerComplaints.csv\"\n",
    "df = pd.read_csv(complaints_path, low_memory=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "129b3034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 65499\n",
      "Positive rate: 0.9772210262752103\n",
      "Date range: 2013-07-22 to 2015-09-02\n"
     ]
    }
   ],
   "source": [
    "# Basic cleanup for the demo\n",
    "df = df.copy()\n",
    "df['Date Received'] = pd.to_datetime(df['Date Received'], errors='coerce')\n",
    "df = df.dropna(subset=['Date Received', 'Timely Response'])\n",
    "\n",
    "y = (df['Timely Response'].astype(str).str.strip().str.lower() == 'yes').astype(int)\n",
    "feature_cols = ['Product Name', 'Sub Product', 'Issue', 'Sub Issue', 'Company', 'State Name', 'Submitted via']\n",
    "X = df[feature_cols]\n",
    "\n",
    "print('Rows:', len(df))\n",
    "print('Positive rate:', float(y.mean()))\n",
    "print('Date range:', df['Date Received'].min().date(), 'to', df['Date Received'].max().date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03cacfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = feature_cols\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), categorical_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "clf = Pipeline(steps=[\n",
    "    ('preprocess', preprocess),\n",
    "    ('model', LogisticRegression(max_iter=200))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ef5e0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random split accuracy: 0.9781\n",
      "Random split ROC-AUC  : 0.9154\n"
     ]
    }
   ],
   "source": [
    "# (1) Random stratified split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "proba = clf.predict_proba(X_test)[:, 1]\n",
    "pred = (proba >= 0.5).astype(int)\n",
    "\n",
    "acc_random = accuracy_score(y_test, pred)\n",
    "auc_random = roc_auc_score(y_test, proba)\n",
    "print('Random split accuracy:', round(acc_random, 4))\n",
    "print('Random split ROC-AUC  :', round(auc_random, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd9f6b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time split accuracy: 0.9705\n",
      "Time split ROC-AUC  : 0.8955\n",
      "Train end date: 2015-01-20\n",
      "Test start date: 2015-01-20\n"
     ]
    }
   ],
   "source": [
    "# (2) Time-based split: train on earlier 80%, test on later 20%\n",
    "df_sorted = df.sort_values('Date Received')\n",
    "X_sorted = df_sorted[feature_cols]\n",
    "y_sorted = (df_sorted['Timely Response'].astype(str).str.strip().str.lower() == 'yes').astype(int)\n",
    "\n",
    "cut = int(0.8 * len(df_sorted))\n",
    "X_train_t, X_test_t = X_sorted.iloc[:cut], X_sorted.iloc[cut:]\n",
    "y_train_t, y_test_t = y_sorted.iloc[:cut], y_sorted.iloc[cut:]\n",
    "\n",
    "clf.fit(X_train_t, y_train_t)\n",
    "proba_t = clf.predict_proba(X_test_t)[:, 1]\n",
    "pred_t = (proba_t >= 0.5).astype(int)\n",
    "\n",
    "acc_time = accuracy_score(y_test_t, pred_t)\n",
    "auc_time = roc_auc_score(y_test_t, proba_t)\n",
    "print('Time split accuracy:', round(acc_time, 4))\n",
    "print('Time split ROC-AUC  :', round(auc_time, 4))\n",
    "print('Train end date:', df_sorted['Date Received'].iloc[cut-1].date())\n",
    "print('Test start date:', df_sorted['Date Received'].iloc[cut].date())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1205ac",
   "metadata": {},
   "source": [
    "### Interpreting the difference\n",
    "\n",
    "If the random split score is higher than the time split score, this is usually not “bad news.” It is evidence that the future is harder than a shuffled snapshot.\n",
    "\n",
    "Typical contributors:\n",
    "\n",
    "- Distribution shift (new products, policy changes).\n",
    "- Time correlation (nearby dates share context).\n",
    "- Changing base rates.\n",
    "\n",
    "If the production system must predict on future periods, a forward-in-time split is the relevant estimate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958aace9",
   "metadata": {},
   "source": [
    "### Rolling/expanding validation with `TimeSeriesSplit`\n",
    "\n",
    "When a single holdout is noisy, use rolling validation.\n",
    "\n",
    "In expanding-window evaluation:\n",
    "\n",
    "$$\\text{Train}_k = \\{t \\le t_k\\},\\quad \\text{Test}_k = \\{t_k < t \\le t_{k+1}\\}.$$\n",
    "\n",
    "This matches the operational reality where you train on all history available up to some date.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5b6d500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: AUC=0.8265 | train_end=2013-12-08 | test=2013-12-08..2014-03-25\n",
      "Fold 2: AUC=0.8235 | train_end=2014-03-25 | test=2014-03-25..2014-07-08\n",
      "Fold 3: AUC=0.8747 | train_end=2014-07-08 | test=2014-07-08..2014-10-20\n",
      "Fold 4: AUC=0.8834 | train_end=2014-10-20 | test=2014-10-20..2015-02-09\n",
      "Fold 5: AUC=0.8959 | train_end=2015-02-09 | test=2015-02-09..2015-09-02\n",
      "Mean AUC: 0.8607962707647356\n",
      "Std  AUC: 0.029990864243419388\n"
     ]
    }
   ],
   "source": [
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "X_ts = X_sorted.reset_index(drop=True)\n",
    "y_ts = y_sorted.reset_index(drop=True)\n",
    "\n",
    "aucs = []\n",
    "for fold, (tr, te) in enumerate(tscv.split(X_ts), start=1):\n",
    "    clf.fit(X_ts.iloc[tr], y_ts.iloc[tr])\n",
    "    proba = clf.predict_proba(X_ts.iloc[te])[:, 1]\n",
    "    auc = roc_auc_score(y_ts.iloc[te], proba)\n",
    "    aucs.append(auc)\n",
    "    train_end = df_sorted.iloc[tr[-1]]['Date Received'].date()\n",
    "    test_start = df_sorted.iloc[te[0]]['Date Received'].date()\n",
    "    test_end = df_sorted.iloc[te[-1]]['Date Received'].date()\n",
    "    print(f'Fold {fold}: AUC={auc:.4f} | train_end={train_end} | test={test_start}..{test_end}')\n",
    "\n",
    "print('Mean AUC:', float(np.mean(aucs)))\n",
    "print('Std  AUC:', float(np.std(aucs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce39dc3",
   "metadata": {},
   "source": [
    "### Temporal guardrails: gaps and label delay\n",
    "\n",
    "If labels occur after some delay (e.g., default after 90 days) or features aggregate over windows, you can inadvertently let information from the label window leak into features.\n",
    "\n",
    "A simple guardrail is a gap $g$:\n",
    "\n",
    "$$\\text{Train} = \\{t \\le t_0\\},\\quad \\text{Gap} = (t_0, t_0 + g],\\quad \\text{Test} = \\{t > t_0 + g\\}.$$\n",
    "\n",
    "Choose $g$ to cover the maximum look-ahead horizon embedded in feature definitions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2d9151",
   "metadata": {},
   "source": [
    "## 3) Group and entity splits\n",
    "\n",
    "Group splits address dependence caused by repeated entities or shared context.\n",
    "\n",
    "Two common deployment questions:\n",
    "\n",
    "- **Generalize to new entities?** (e.g., new patients, new hosts, new devices) → group-disjoint train/test.\n",
    "- **Forecast future for existing entities?** (e.g., next month for the same customers) → time split within entity, possibly with additional gaps.\n",
    "\n",
    "The split must match which of these is operationally true.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dc1e5b",
   "metadata": {},
   "source": [
    "### A short bias argument\n",
    "\n",
    "Let $G$ be an entity ID and suppose there is an unobserved entity effect $\\alpha_G$.\n",
    "A simple model is:\n",
    "\n",
    "$$Y = \\beta^\\top X + \\alpha_G + \\epsilon.$$\n",
    "\n",
    "If train and test share the same $G$, the learner can partially infer $\\alpha_G$ from training rows, making prediction easier on test rows with that same entity.\n",
    "This yields a test estimate that is biased toward the “seen entity” regime.\n",
    "\n",
    "If the production requirement is performance on unseen entities, enforce disjointness of $G$ across splits.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdad22f",
   "metadata": {},
   "source": [
    "### Example B: Host-level leakage in listings data\n",
    "\n",
    "Dataset: `listings.csv`. Entity: `host_id`.\n",
    "Task: predict `room_type`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09dc76cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>host_id</th>\n",
       "      <th>host_name</th>\n",
       "      <th>neighbourhood_group</th>\n",
       "      <th>neighbourhood</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>room_type</th>\n",
       "      <th>price</th>\n",
       "      <th>minimum_nights</th>\n",
       "      <th>number_of_reviews</th>\n",
       "      <th>last_review</th>\n",
       "      <th>reviews_per_month</th>\n",
       "      <th>calculated_host_listings_count</th>\n",
       "      <th>availability_365</th>\n",
       "      <th>number_of_reviews_ltm</th>\n",
       "      <th>license</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13913</td>\n",
       "      <td>Holiday London DB Room Let-on going</td>\n",
       "      <td>54730</td>\n",
       "      <td>Alina</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Islington</td>\n",
       "      <td>51.56861</td>\n",
       "      <td>-0.11270</td>\n",
       "      <td>Private room</td>\n",
       "      <td>57.0</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>2025-02-09</td>\n",
       "      <td>0.29</td>\n",
       "      <td>3</td>\n",
       "      <td>344</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15400</td>\n",
       "      <td>Bright Chelsea  Apartment. Chelsea!</td>\n",
       "      <td>60302</td>\n",
       "      <td>Philippa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Kensington and Chelsea</td>\n",
       "      <td>51.48780</td>\n",
       "      <td>-0.16813</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>96</td>\n",
       "      <td>2024-04-28</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17402</td>\n",
       "      <td>Very Central Modern 3-Bed/2 Bath By Oxford St W1</td>\n",
       "      <td>67564</td>\n",
       "      <td>Liz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Westminster</td>\n",
       "      <td>51.52195</td>\n",
       "      <td>-0.14094</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>510.0</td>\n",
       "      <td>3</td>\n",
       "      <td>56</td>\n",
       "      <td>2024-02-19</td>\n",
       "      <td>0.33</td>\n",
       "      <td>5</td>\n",
       "      <td>293</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24328</td>\n",
       "      <td>Battersea live/work artist house</td>\n",
       "      <td>41759</td>\n",
       "      <td>Joe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wandsworth</td>\n",
       "      <td>51.47072</td>\n",
       "      <td>-0.16266</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>213.0</td>\n",
       "      <td>90</td>\n",
       "      <td>94</td>\n",
       "      <td>2022-07-19</td>\n",
       "      <td>0.54</td>\n",
       "      <td>1</td>\n",
       "      <td>194</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31036</td>\n",
       "      <td>Bright  compact 1 Bedroom Apartment Brick Lane</td>\n",
       "      <td>133271</td>\n",
       "      <td>Hendryks</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tower Hamlets</td>\n",
       "      <td>51.52425</td>\n",
       "      <td>-0.06997</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2</td>\n",
       "      <td>126</td>\n",
       "      <td>2025-02-20</td>\n",
       "      <td>0.70</td>\n",
       "      <td>8</td>\n",
       "      <td>353</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              name  host_id host_name  neighbourhood_group           neighbourhood  latitude  longitude  \\\n",
       "0  13913               Holiday London DB Room Let-on going    54730     Alina                  NaN               Islington  51.56861   -0.11270   \n",
       "1  15400               Bright Chelsea  Apartment. Chelsea!    60302  Philippa                  NaN  Kensington and Chelsea  51.48780   -0.16813   \n",
       "2  17402  Very Central Modern 3-Bed/2 Bath By Oxford St W1    67564       Liz                  NaN             Westminster  51.52195   -0.14094   \n",
       "3  24328                  Battersea live/work artist house    41759       Joe                  NaN              Wandsworth  51.47072   -0.16266   \n",
       "4  31036    Bright  compact 1 Bedroom Apartment Brick Lane   133271  Hendryks                  NaN           Tower Hamlets  51.52425   -0.06997   \n",
       "\n",
       "         room_type  price  minimum_nights  number_of_reviews last_review  reviews_per_month  calculated_host_listings_count  availability_365  \\\n",
       "0     Private room   57.0               1                 51  2025-02-09               0.29                               3               344   \n",
       "1  Entire home/apt    NaN               4                 96  2024-04-28               0.52                               1                11   \n",
       "2  Entire home/apt  510.0               3                 56  2024-02-19               0.33                               5               293   \n",
       "3  Entire home/apt  213.0              90                 94  2022-07-19               0.54                               1               194   \n",
       "4  Entire home/apt  100.0               2                126  2025-02-20               0.70                               8               353   \n",
       "\n",
       "   number_of_reviews_ltm  license  \n",
       "0                     10      NaN  \n",
       "1                      2      NaN  \n",
       "2                      0      NaN  \n",
       "3                      0      NaN  \n",
       "4                      3      NaN  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listings_path = \"../../../Datasets/Regression/listings.csv\"\n",
    "ldf = pd.read_csv(listings_path, low_memory=False)\n",
    "ldf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2656f814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 94559\n",
      "Unique hosts: 55395\n",
      "Class counts (top):\n",
      "room_type\n",
      "Entire home/apt    60750\n",
      "Private room       33487\n",
      "Shared room          164\n",
      "Hotel room           158\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "ldf = ldf.copy()\n",
    "ldf = ldf.dropna(subset=['host_id', 'room_type'])\n",
    "\n",
    "features = ['neighbourhood', 'latitude', 'longitude', 'price', 'minimum_nights', 'number_of_reviews', 'reviews_per_month', 'availability_365']\n",
    "for col in ['price', 'reviews_per_month']:\n",
    "    ldf[col] = pd.to_numeric(ldf[col], errors='coerce')\n",
    "\n",
    "X2 = ldf[features]\n",
    "y2 = ldf['room_type'].astype(str)\n",
    "groups = ldf['host_id'].astype(str)\n",
    "\n",
    "print('Rows:', len(ldf))\n",
    "print('Unique hosts:', groups.nunique())\n",
    "print('Class counts (top):')\n",
    "print(y2.value_counts().head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49295e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = ['latitude', 'longitude', 'price', 'minimum_nights', 'number_of_reviews', 'reviews_per_month', 'availability_365']\n",
    "cat_features = ['neighbourhood']\n",
    "\n",
    "preprocess2 = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline(steps=[('imputer', SimpleImputer(strategy='median')),\n",
    "                               ('scaler', StandardScaler())]), num_features),\n",
    "        ('cat', Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                               ('onehot', OneHotEncoder(handle_unknown='ignore'))]), cat_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "clf2 = Pipeline(steps=[\n",
    "    ('preprocess', preprocess2),\n",
    "    ('model', LogisticRegression(max_iter=300))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8a77ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random split accuracy: 0.7474\n",
      "Hosts shared train/test: 5126\n",
      "Train hosts: 46337 | Test hosts: 14184\n"
     ]
    }
   ],
   "source": [
    "# Random split\n",
    "X_tr, X_te, y_tr, y_te, g_tr, g_te = train_test_split(\n",
    "    X2, y2, groups, test_size=0.2, random_state=42, stratify=y2\n",
    ")\n",
    "clf2.fit(X_tr, y_tr)\n",
    "pred = clf2.predict(X_te)\n",
    "acc = accuracy_score(y_te, pred)\n",
    "\n",
    "shared_hosts = len(set(g_tr) & set(g_te))\n",
    "print('Random split accuracy:', round(acc, 4))\n",
    "print('Hosts shared train/test:', shared_hosts)\n",
    "print('Train hosts:', len(set(g_tr)), '| Test hosts:', len(set(g_te)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbb35b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group split accuracy: 0.7529\n",
      "Hosts shared train/test: 0\n",
      "Train hosts: 44316 | Test hosts: 11079\n"
     ]
    }
   ],
   "source": [
    "# Group split (host-disjoint)\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "tr_idx, te_idx = next(gss.split(X2, y2, groups=groups))\n",
    "\n",
    "X_trg, X_teg = X2.iloc[tr_idx], X2.iloc[te_idx]\n",
    "y_trg, y_teg = y2.iloc[tr_idx], y2.iloc[te_idx]\n",
    "g_trg, g_teg = groups.iloc[tr_idx], groups.iloc[te_idx]\n",
    "\n",
    "clf2.fit(X_trg, y_trg)\n",
    "pred_g = clf2.predict(X_teg)\n",
    "acc_g = accuracy_score(y_teg, pred_g)\n",
    "\n",
    "shared_hosts_g = len(set(g_trg) & set(g_teg))\n",
    "print('Group split accuracy:', round(acc_g, 4))\n",
    "print('Hosts shared train/test:', shared_hosts_g)\n",
    "print('Train hosts:', len(set(g_trg)), '| Test hosts:', len(set(g_teg)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9af2be9",
   "metadata": {},
   "source": [
    "### Leakage forensics: overlap and near-duplicates\n",
    "\n",
    "Even when entity IDs are disjoint, near-duplicates can leak information (for example, the same item replicated with small edits).\n",
    "\n",
    "A simple, practical check is to hash a subset of stable columns and measure how many hashes appear in both train and test.\n",
    "This is not perfect, but it often catches obvious duplication.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "570711cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approx duplicate signatures (random split): 556\n",
      "Approx duplicate signatures (group split) : 59\n"
     ]
    }
   ],
   "source": [
    "def overlap_count(a, b):\n",
    "    return len(set(a) & set(b))\n",
    "\n",
    "# Create a rough signature for listings (you can adjust columns depending on your data)\n",
    "sig_cols = ['neighbourhood', 'latitude', 'longitude', 'minimum_nights']\n",
    "sig = X2[sig_cols].copy()\n",
    "sig['latitude'] = sig['latitude'].round(5)\n",
    "sig['longitude'] = sig['longitude'].round(5)\n",
    "signature = pd.util.hash_pandas_object(sig, index=False)\n",
    "\n",
    "# Compare duplication under random split vs group split\n",
    "sig_tr = signature.iloc[X_tr.index]\n",
    "sig_te = signature.iloc[X_te.index]\n",
    "dup_random = overlap_count(sig_tr, sig_te)\n",
    "\n",
    "sig_trg = signature.iloc[tr_idx]\n",
    "sig_teg = signature.iloc[te_idx]\n",
    "dup_group = overlap_count(sig_trg, sig_teg)\n",
    "\n",
    "print('Approx duplicate signatures (random split):', dup_random)\n",
    "print('Approx duplicate signatures (group split) :', dup_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced8da09",
   "metadata": {},
   "source": [
    "### Group-aware cross-validation with `GroupKFold`\n",
    "\n",
    "When you need multiple folds while keeping entities disjoint, use `GroupKFold`.\n",
    "This produces variance estimates and reduces dependence on one arbitrary split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00896c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: acc=0.7404 | shared hosts=0 | test hosts=11078\n",
      "Fold 2: acc=0.7416 | shared hosts=0 | test hosts=11079\n",
      "Fold 3: acc=0.7348 | shared hosts=0 | test hosts=11079\n",
      "Fold 4: acc=0.7481 | shared hosts=0 | test hosts=11080\n",
      "Fold 5: acc=0.7515 | shared hosts=0 | test hosts=11079\n",
      "Mean acc: 0.7432714839285025\n",
      "Std  acc: 0.00591220648589423\n"
     ]
    }
   ],
   "source": [
    "gkf = GroupKFold(n_splits=5)\n",
    "accs = []\n",
    "for fold, (tr, te) in enumerate(gkf.split(X2, y2, groups=groups), start=1):\n",
    "    clf2.fit(X2.iloc[tr], y2.iloc[tr])\n",
    "    pred = clf2.predict(X2.iloc[te])\n",
    "    acc = accuracy_score(y2.iloc[te], pred)\n",
    "    accs.append(acc)\n",
    "    shared = len(set(groups.iloc[tr]) & set(groups.iloc[te]))\n",
    "    print(f'Fold {fold}: acc={acc:.4f} | shared hosts={shared} | test hosts={groups.iloc[te].nunique()}')\n",
    "\n",
    "print('Mean acc:', float(np.mean(accs)))\n",
    "print('Std  acc:', float(np.std(accs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6cf686",
   "metadata": {},
   "source": [
    "## 4) Combined structure: time + entity (panel data)\n",
    "\n",
    "Many real datasets are **panel data**: repeated measurements for each entity over time (stores over weeks, states over years, machines over cycles).\n",
    "\n",
    "In panel data you often must decide between two different evaluation targets:\n",
    "\n",
    "- **Unseen-entity generalization:** predict for entities never seen before.\n",
    "- **Within-entity forecasting:** predict future for entities already observed historically.\n",
    "\n",
    "These require different splits, and the performance numbers answer different questions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4367eb41",
   "metadata": {},
   "source": [
    "### Example C: Education panel data (`states_all.csv`)\n",
    "\n",
    "Dataset: `states_all.csv` with columns `STATE` and `YEAR`.\n",
    "\n",
    "Task: predict `AVG_MATH_8_SCORE` from funding/expenditure variables.\n",
    "\n",
    "We will compare three splits:\n",
    "\n",
    "1. Random row split (often optimistic because the same state appears in train and test).\n",
    "2. Group split by `STATE` (unseen states).\n",
    "3. Within-state temporal split (forecast future years for the same states).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5ab6505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PRIMARY_KEY</th>\n",
       "      <th>STATE</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>ENROLL</th>\n",
       "      <th>TOTAL_REVENUE</th>\n",
       "      <th>FEDERAL_REVENUE</th>\n",
       "      <th>STATE_REVENUE</th>\n",
       "      <th>LOCAL_REVENUE</th>\n",
       "      <th>TOTAL_EXPENDITURE</th>\n",
       "      <th>INSTRUCTION_EXPENDITURE</th>\n",
       "      <th>SUPPORT_SERVICES_EXPENDITURE</th>\n",
       "      <th>OTHER_EXPENDITURE</th>\n",
       "      <th>CAPITAL_OUTLAY_EXPENDITURE</th>\n",
       "      <th>GRADES_PK_G</th>\n",
       "      <th>GRADES_KG_G</th>\n",
       "      <th>GRADES_4_G</th>\n",
       "      <th>GRADES_8_G</th>\n",
       "      <th>GRADES_12_G</th>\n",
       "      <th>GRADES_1_8_G</th>\n",
       "      <th>GRADES_9_12_G</th>\n",
       "      <th>GRADES_ALL_G</th>\n",
       "      <th>AVG_MATH_4_SCORE</th>\n",
       "      <th>AVG_MATH_8_SCORE</th>\n",
       "      <th>AVG_READING_4_SCORE</th>\n",
       "      <th>AVG_READING_8_SCORE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1992_ALABAMA</td>\n",
       "      <td>ALABAMA</td>\n",
       "      <td>1992</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2678885.0</td>\n",
       "      <td>304177.0</td>\n",
       "      <td>1659028.0</td>\n",
       "      <td>715680.0</td>\n",
       "      <td>2653798.0</td>\n",
       "      <td>1481703.0</td>\n",
       "      <td>735036.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>174053.0</td>\n",
       "      <td>8224.0</td>\n",
       "      <td>55460.0</td>\n",
       "      <td>57948.0</td>\n",
       "      <td>58025.0</td>\n",
       "      <td>41167.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>731634.0</td>\n",
       "      <td>208.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>207.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1992_ALASKA</td>\n",
       "      <td>ALASKA</td>\n",
       "      <td>1992</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1049591.0</td>\n",
       "      <td>106780.0</td>\n",
       "      <td>720711.0</td>\n",
       "      <td>222100.0</td>\n",
       "      <td>972488.0</td>\n",
       "      <td>498362.0</td>\n",
       "      <td>350902.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37451.0</td>\n",
       "      <td>2371.0</td>\n",
       "      <td>10152.0</td>\n",
       "      <td>9748.0</td>\n",
       "      <td>8789.0</td>\n",
       "      <td>6714.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>122487.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1992_ARIZONA</td>\n",
       "      <td>ARIZONA</td>\n",
       "      <td>1992</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3258079.0</td>\n",
       "      <td>297888.0</td>\n",
       "      <td>1369815.0</td>\n",
       "      <td>1590376.0</td>\n",
       "      <td>3401580.0</td>\n",
       "      <td>1435908.0</td>\n",
       "      <td>1007732.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>609114.0</td>\n",
       "      <td>2544.0</td>\n",
       "      <td>53497.0</td>\n",
       "      <td>55433.0</td>\n",
       "      <td>49081.0</td>\n",
       "      <td>37410.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>673477.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>265.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1992_ARKANSAS</td>\n",
       "      <td>ARKANSAS</td>\n",
       "      <td>1992</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1711959.0</td>\n",
       "      <td>178571.0</td>\n",
       "      <td>958785.0</td>\n",
       "      <td>574603.0</td>\n",
       "      <td>1743022.0</td>\n",
       "      <td>964323.0</td>\n",
       "      <td>483488.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>145212.0</td>\n",
       "      <td>808.0</td>\n",
       "      <td>33511.0</td>\n",
       "      <td>34632.0</td>\n",
       "      <td>36011.0</td>\n",
       "      <td>27651.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>441490.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1992_CALIFORNIA</td>\n",
       "      <td>CALIFORNIA</td>\n",
       "      <td>1992</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26260025.0</td>\n",
       "      <td>2072470.0</td>\n",
       "      <td>16546514.0</td>\n",
       "      <td>7641041.0</td>\n",
       "      <td>27138832.0</td>\n",
       "      <td>14358922.0</td>\n",
       "      <td>8520926.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2044688.0</td>\n",
       "      <td>59067.0</td>\n",
       "      <td>431763.0</td>\n",
       "      <td>418418.0</td>\n",
       "      <td>363296.0</td>\n",
       "      <td>270675.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5254844.0</td>\n",
       "      <td>208.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PRIMARY_KEY       STATE  YEAR  ENROLL  TOTAL_REVENUE  FEDERAL_REVENUE  STATE_REVENUE  LOCAL_REVENUE  TOTAL_EXPENDITURE  INSTRUCTION_EXPENDITURE  \\\n",
       "0     1992_ALABAMA     ALABAMA  1992     NaN      2678885.0         304177.0      1659028.0       715680.0          2653798.0                1481703.0   \n",
       "1      1992_ALASKA      ALASKA  1992     NaN      1049591.0         106780.0       720711.0       222100.0           972488.0                 498362.0   \n",
       "2     1992_ARIZONA     ARIZONA  1992     NaN      3258079.0         297888.0      1369815.0      1590376.0          3401580.0                1435908.0   \n",
       "3    1992_ARKANSAS    ARKANSAS  1992     NaN      1711959.0         178571.0       958785.0       574603.0          1743022.0                 964323.0   \n",
       "4  1992_CALIFORNIA  CALIFORNIA  1992     NaN     26260025.0        2072470.0     16546514.0      7641041.0         27138832.0               14358922.0   \n",
       "\n",
       "   SUPPORT_SERVICES_EXPENDITURE  OTHER_EXPENDITURE  CAPITAL_OUTLAY_EXPENDITURE  GRADES_PK_G  GRADES_KG_G  GRADES_4_G  GRADES_8_G  GRADES_12_G  GRADES_1_8_G  \\\n",
       "0                      735036.0                NaN                    174053.0       8224.0      55460.0     57948.0     58025.0      41167.0           NaN   \n",
       "1                      350902.0                NaN                     37451.0       2371.0      10152.0      9748.0      8789.0       6714.0           NaN   \n",
       "2                     1007732.0                NaN                    609114.0       2544.0      53497.0     55433.0     49081.0      37410.0           NaN   \n",
       "3                      483488.0                NaN                    145212.0        808.0      33511.0     34632.0     36011.0      27651.0           NaN   \n",
       "4                     8520926.0                NaN                   2044688.0      59067.0     431763.0    418418.0    363296.0     270675.0           NaN   \n",
       "\n",
       "   GRADES_9_12_G  GRADES_ALL_G  AVG_MATH_4_SCORE  AVG_MATH_8_SCORE  AVG_READING_4_SCORE  AVG_READING_8_SCORE  \n",
       "0            NaN      731634.0             208.0             252.0                207.0                  NaN  \n",
       "1            NaN      122487.0               NaN               NaN                  NaN                  NaN  \n",
       "2            NaN      673477.0             215.0             265.0                209.0                  NaN  \n",
       "3            NaN      441490.0             210.0             256.0                211.0                  NaN  \n",
       "4            NaN     5254844.0             208.0             261.0                202.0                  NaN  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states_path = \"../../../Datasets/Regression/states_all.csv\"\n",
    "sdf = pd.read_csv(states_path, low_memory=False)\n",
    "sdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f427b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 602\n",
      "States: 53\n",
      "Year range: 1990 to 2019\n"
     ]
    }
   ],
   "source": [
    "sdf = sdf.copy()\n",
    "sdf['YEAR'] = pd.to_numeric(sdf['YEAR'], errors='coerce')\n",
    "sdf = sdf.dropna(subset=['STATE', 'YEAR', 'AVG_MATH_8_SCORE'])\n",
    "\n",
    "target = 'AVG_MATH_8_SCORE'\n",
    "group_col = 'STATE'\n",
    "\n",
    "num_cols = [\n",
    "    'ENROLL', 'TOTAL_REVENUE', 'FEDERAL_REVENUE', 'STATE_REVENUE', 'LOCAL_REVENUE',\n",
    "    'TOTAL_EXPENDITURE', 'INSTRUCTION_EXPENDITURE', 'SUPPORT_SERVICES_EXPENDITURE', 'CAPITAL_OUTLAY_EXPENDITURE'\n",
    "]\n",
    "X4 = sdf[num_cols].apply(pd.to_numeric, errors='coerce')\n",
    "y4 = pd.to_numeric(sdf[target], errors='coerce')\n",
    "g4 = sdf[group_col].astype(str)\n",
    "t4 = sdf['YEAR'].astype(int)\n",
    "\n",
    "mask = y4.notna()\n",
    "X4, y4, g4, t4 = X4.loc[mask], y4.loc[mask], g4.loc[mask], t4.loc[mask]\n",
    "\n",
    "print('Rows:', len(X4))\n",
    "print('States:', g4.nunique())\n",
    "print('Year range:', int(t4.min()), 'to', int(t4.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9997149",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_pipe = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', Ridge())\n",
    "])\n",
    "\n",
    "def eval_regression(y_true, y_pred):\n",
    "    rmse = mean_squared_error(y_true, y_pred) ** 0.5\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return rmse, r2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a1d0782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random row split | RMSE: 9.193 | R^2: 0.092\n",
      "Shared states: 48\n",
      "Shared years  : 12\n"
     ]
    }
   ],
   "source": [
    "# (1) Random row split\n",
    "X_tr, X_te, y_tr, y_te, g_tr, g_te, t_tr, t_te = train_test_split(\n",
    "    X4, y4, g4, t4, test_size=0.2, random_state=42\n",
    ")\n",
    "reg_pipe.fit(X_tr, y_tr)\n",
    "pred = reg_pipe.predict(X_te)\n",
    "rmse, r2 = eval_regression(y_te, pred)\n",
    "print('Random row split | RMSE:', round(rmse, 3), '| R^2:', round(r2, 3))\n",
    "print('Shared states:', len(set(g_tr) & set(g_te)))\n",
    "print('Shared years  :', len(set(t_tr) & set(t_te)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2eeb2734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group split (STATE) | RMSE: 9.889 | R^2: 0.164\n",
      "Shared states: 0\n"
     ]
    }
   ],
   "source": [
    "# (2) Group split by STATE (unseen states)\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "tr_idx, te_idx = next(gss.split(X4, y4, groups=g4))\n",
    "X_trg, X_teg = X4.iloc[tr_idx], X4.iloc[te_idx]\n",
    "y_trg, y_teg = y4.iloc[tr_idx], y4.iloc[te_idx]\n",
    "g_trg, g_teg = g4.iloc[tr_idx], g4.iloc[te_idx]\n",
    "\n",
    "reg_pipe.fit(X_trg, y_trg)\n",
    "pred = reg_pipe.predict(X_teg)\n",
    "rmse, r2 = eval_regression(y_teg, pred)\n",
    "print('Group split (STATE) | RMSE:', round(rmse, 3), '| R^2:', round(r2, 3))\n",
    "print('Shared states:', len(set(g_trg) & set(g_teg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "12702d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Within-state time split | RMSE: 8.322 | R^2: -0.604\n",
      "States with time-order violations: 0\n"
     ]
    }
   ],
   "source": [
    "# (3) Within-state temporal split: for each state, keep its latest 20% years as test\n",
    "sdf_work = pd.DataFrame({'state': g4.values, 'year': t4.values})\n",
    "sdf_work['idx'] = np.arange(len(sdf_work))\n",
    "\n",
    "test_mask = np.zeros(len(sdf_work), dtype=bool)\n",
    "for state, sub in sdf_work.groupby('state'):\n",
    "    years = np.sort(sub['year'].unique())\n",
    "    if len(years) < 5:\n",
    "        continue\n",
    "    cut_year = years[int(np.floor(0.8 * len(years)))]\n",
    "    # test = years strictly greater than cut_year\n",
    "    sub_idx = sub.loc[sub['year'] > cut_year, 'idx'].values\n",
    "    test_mask[sub_idx] = True\n",
    "\n",
    "tr_idx = np.where(~test_mask)[0]\n",
    "te_idx = np.where(test_mask)[0]\n",
    "\n",
    "X_trt, X_tet = X4.iloc[tr_idx], X4.iloc[te_idx]\n",
    "y_trt, y_tet = y4.iloc[tr_idx], y4.iloc[te_idx]\n",
    "\n",
    "reg_pipe.fit(X_trt, y_trt)\n",
    "pred = reg_pipe.predict(X_tet)\n",
    "rmse, r2 = eval_regression(y_tet, pred)\n",
    "print('Within-state time split | RMSE:', round(rmse, 3), '| R^2:', round(r2, 3))\n",
    "\n",
    "# Verify time ordering holds within each state\n",
    "violations = 0\n",
    "for state, sub in sdf_work.groupby('state'):\n",
    "    tr_years = t4.iloc[tr_idx][g4.iloc[tr_idx] == state]\n",
    "    te_years = t4.iloc[te_idx][g4.iloc[te_idx] == state]\n",
    "    if len(tr_years) and len(te_years) and tr_years.max() >= te_years.min():\n",
    "        violations += 1\n",
    "print('States with time-order violations:', violations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292ed0a3",
   "metadata": {},
   "source": [
    "### Reading panel split results\n",
    "\n",
    "These three numbers answer different questions:\n",
    "\n",
    "- Random row split: “How well can we predict if we mix states and years?” (often optimistic).\n",
    "- Group split by state: “How well do we generalize to entirely new states?” (harder).\n",
    "- Within-state time split: “How well do we forecast future years for the same states?” (deployment-like for tracking existing entities).\n",
    "\n",
    "Choosing the wrong split can lead to the wrong product decision.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b9211e",
   "metadata": {},
   "source": [
    "## 5) Preprocessing hygiene: fit on train only\n",
    "\n",
    "Preprocessing steps (imputation, scaling, encoding) estimate parameters from data.\n",
    "\n",
    "For example, standardization uses:\n",
    "\n",
    "$$\\tilde{x} = \\frac{x - \\mu}{\\sigma},\\quad \\mu = \\frac{1}{n}\\sum_{i=1}^n x_i,\\quad \\sigma^2 = \\frac{1}{n}\\sum_{i=1}^n (x_i - \\mu)^2.$$\n",
    "\n",
    "If $\\mu$ and $\\sigma$ are computed using test rows, then the training procedure has indirectly used test information.\n",
    "\n",
    "Pipelines ensure that parameter estimation is nested inside the split.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a296af44",
   "metadata": {},
   "source": [
    "### Example D: Scaling leakage on the diabetes dataset\n",
    "\n",
    "Task: predict `classification` from numeric features.\n",
    "We compare a leaky scaling pattern with the correct pattern.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "52125682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>Diabetic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>Non-Diabetic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>Diabetic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>Non-Diabetic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>Diabetic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  DiabetesPedigreeFunction  Age classification\n",
       "0            6      148             72             35        0  33.6                     0.627   50       Diabetic\n",
       "1            1       85             66             29        0  26.6                     0.351   31   Non-Diabetic\n",
       "2            8      183             64              0        0  23.3                     0.672   32       Diabetic\n",
       "3            1       89             66             23       94  28.1                     0.167   21   Non-Diabetic\n",
       "4            0      137             40             35      168  43.1                     2.288   33       Diabetic"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes_path = \"../../../Datasets/Classification/diabetes.csv\"\n",
    "ddf = pd.read_csv(diabetes_path)\n",
    "ddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0991d974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 576 | Test size: 192\n"
     ]
    }
   ],
   "source": [
    "ddf = ddf.copy()\n",
    "y5 = (ddf['classification'].astype(str).str.strip().str.lower() == 'diabetic').astype(int)\n",
    "X5 = ddf.drop(columns=['classification'])\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X5, y5, test_size=0.25, random_state=42, stratify=y5)\n",
    "print('Train size:', len(X_tr), '| Test size:', len(X_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "10582ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC (leaky scaling): 0.832\n"
     ]
    }
   ],
   "source": [
    "# Leaky: fit scaler on all data\n",
    "scaler_bad = StandardScaler().fit(X5)\n",
    "X_tr_bad = scaler_bad.transform(X_tr)\n",
    "X_te_bad = scaler_bad.transform(X_te)\n",
    "\n",
    "m_bad = LogisticRegression(max_iter=600)\n",
    "m_bad.fit(X_tr_bad, y_tr)\n",
    "auc_bad = roc_auc_score(y_te, m_bad.predict_proba(X_te_bad)[:, 1])\n",
    "print('ROC-AUC (leaky scaling):', round(auc_bad, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1e990bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC (correct scaling): 0.832\n"
     ]
    }
   ],
   "source": [
    "# Correct: fit scaler on train only\n",
    "scaler_ok = StandardScaler().fit(X_tr)\n",
    "X_tr_ok = scaler_ok.transform(X_tr)\n",
    "X_te_ok = scaler_ok.transform(X_te)\n",
    "\n",
    "m_ok = LogisticRegression(max_iter=600)\n",
    "m_ok.fit(X_tr_ok, y_tr)\n",
    "auc_ok = roc_auc_score(y_te, m_ok.predict_proba(X_te_ok)[:, 1])\n",
    "print('ROC-AUC (correct scaling):', round(auc_ok, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "293a8810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC (pipeline): 0.832\n"
     ]
    }
   ],
   "source": [
    "# Best practice: Pipeline\n",
    "pipe = Pipeline([('scaler', StandardScaler()), ('lr', LogisticRegression(max_iter=600))])\n",
    "pipe.fit(X_tr, y_tr)\n",
    "auc_pipe = roc_auc_score(y_te, pipe.predict_proba(X_te)[:, 1])\n",
    "print('ROC-AUC (pipeline):', round(auc_pipe, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de003ef2",
   "metadata": {},
   "source": [
    "## 6) Target encoding: avoiding leakage via cross-fitting\n",
    "\n",
    "Target encoding can be powerful for high-cardinality categoricals, but it is also a high-risk leakage vector.\n",
    "\n",
    "Naive target encoding uses the entire dataset to compute category means:\n",
    "\n",
    "$$\\widehat{m}(c) = \\frac{1}{|\\{i : C_i=c\\}|}\\sum_{i:C_i=c} Y_i.$$\n",
    "\n",
    "If test rows contribute to $\\widehat{m}(c)$, the evaluation becomes optimistic.\n",
    "\n",
    "Cross-fitting approximates an out-of-sample encoding for training rows, making the evaluation honest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c956d5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_encode_crossfit(train_col: pd.Series, y: pd.Series, n_splits: int = 5, smoothing: float = 20.0, random_state: int = 42):\n",
    "    \"\"\"Cross-fitted target encoding for a single categorical column.\n",
    "    Returns: encoded_train (aligned to train_col index), enc_map (means on full train), global_mean\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    global_mean = float(y.mean())\n",
    "    enc = pd.Series(index=train_col.index, dtype=float)\n",
    "    for tr_idx, te_idx in kf.split(train_col):\n",
    "        tr_c = train_col.iloc[tr_idx]\n",
    "        tr_y = y.iloc[tr_idx]\n",
    "        stats = tr_y.groupby(tr_c).agg(['mean', 'count'])\n",
    "        smooth = (stats['count'] * stats['mean'] + smoothing * global_mean) / (stats['count'] + smoothing)\n",
    "        te_c = train_col.iloc[te_idx]\n",
    "        enc.iloc[te_idx] = te_c.map(smooth).fillna(global_mean).astype(float)\n",
    "    full_stats = y.groupby(train_col).agg(['mean', 'count'])\n",
    "    full_smooth = (full_stats['count'] * full_stats['mean'] + smoothing * global_mean) / (full_stats['count'] + smoothing)\n",
    "    return enc, full_smooth, global_mean\n",
    "\n",
    "def target_encode_apply(col: pd.Series, enc_map: pd.Series, global_mean: float):\n",
    "    return col.map(enc_map).fillna(global_mean).astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "99c8119a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rows: 48000 | Test rows: 12000\n",
      "Unique companies in train: 1710\n"
     ]
    }
   ],
   "source": [
    "df_small = df_sorted.tail(60000).copy()\n",
    "df_small = df_small.dropna(subset=['Company', 'Timely Response'])\n",
    "y6 = (df_small['Timely Response'].astype(str).str.strip().str.lower() == 'yes').astype(int)\n",
    "X6 = df_small[['Company', 'State Name', 'Submitted via', 'Product Name']].copy()\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X6, y6, test_size=0.2, random_state=42, stratify=y6)\n",
    "print('Train rows:', len(X_tr), '| Test rows:', len(X_te))\n",
    "print('Unique companies in train:', X_tr['Company'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e9e867a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC (leaky target encoding): 0.9417\n"
     ]
    }
   ],
   "source": [
    "# Leaky encoding (do NOT use in real work)\n",
    "global_mean_all = float(pd.concat([y_tr, y_te]).mean())\n",
    "tmp = pd.DataFrame({'Company': pd.concat([X_tr['Company'], X_te['Company']]).values,\n",
    "                    'y': pd.concat([y_tr, y_te]).values})\n",
    "means_all = tmp.groupby('Company')['y'].mean()\n",
    "\n",
    "X_tr_leak = X_tr.copy(); X_te_leak = X_te.copy()\n",
    "X_tr_leak['Company_te'] = X_tr_leak['Company'].map(means_all).fillna(global_mean_all)\n",
    "X_te_leak['Company_te'] = X_te_leak['Company'].map(means_all).fillna(global_mean_all)\n",
    "\n",
    "pre = ColumnTransformer([\n",
    "    ('num', 'passthrough', ['Company_te']),\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore'), ['State Name', 'Submitted via', 'Product Name']),\n",
    "])\n",
    "m = Pipeline([('pre', pre), ('lr', LogisticRegression(max_iter=300))])\n",
    "m.fit(X_tr_leak[['Company_te', 'State Name', 'Submitted via', 'Product Name']], y_tr)\n",
    "auc_leaky = roc_auc_score(y_te, m.predict_proba(X_te_leak[['Company_te', 'State Name', 'Submitted via', 'Product Name']])[:, 1])\n",
    "print('ROC-AUC (leaky target encoding):', round(auc_leaky, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3f0f6b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC (cross-fitted target encoding): 0.8669\n"
     ]
    }
   ],
   "source": [
    "# Cross-fitted encoding (safe)\n",
    "enc_tr, enc_map, gmean = target_encode_crossfit(X_tr['Company'], y_tr, n_splits=5, smoothing=50.0)\n",
    "X_tr_safe = X_tr.copy(); X_te_safe = X_te.copy()\n",
    "X_tr_safe['Company_te'] = enc_tr\n",
    "X_te_safe['Company_te'] = target_encode_apply(X_te_safe['Company'], enc_map, gmean)\n",
    "\n",
    "m2 = Pipeline([('pre', pre), ('lr', LogisticRegression(max_iter=300))])\n",
    "m2.fit(X_tr_safe[['Company_te', 'State Name', 'Submitted via', 'Product Name']], y_tr)\n",
    "auc_safe = roc_auc_score(y_te, m2.predict_proba(X_te_safe[['Company_te', 'State Name', 'Submitted via', 'Product Name']])[:, 1])\n",
    "print('ROC-AUC (cross-fitted target encoding):', round(auc_safe, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa42e230",
   "metadata": {},
   "source": [
    "## 7) Validation leakage and honest model selection\n",
    "\n",
    "A common process failure is to iterate on the model while repeatedly checking the test set.\n",
    "This makes the test set act like a validation set, and the final reported metric becomes optimistic.\n",
    "\n",
    "A safer pattern:\n",
    "\n",
    "1. Hold out a test set (time-aware or group-aware).\n",
    "2. On the remaining data, run cross-validation (or a validation split) for hyperparameter search.\n",
    "3. Fit the selected model on all non-test data.\n",
    "4. Evaluate once on the test set.\n",
    "\n",
    "Below is a small example using the diabetes dataset, showing a grid search that never touches the test labels during selection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "642d555e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV AUC: 0.8249\n",
      "Best params: {'lr__C': 0.05, 'lr__penalty': 'l2', 'lr__solver': 'lbfgs'}\n",
      "Test AUC: 0.8301\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "ddf = pd.read_csv(diabetes_path)\n",
    "y7 = (ddf['classification'].astype(str).str.strip().str.lower() == 'diabetic').astype(int)\n",
    "X7 = ddf.drop(columns=['classification'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X7, y7, test_size=0.25, random_state=42, stratify=y7)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('lr', LogisticRegression(max_iter=800))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'lr__C': [0.05, 0.1, 0.3, 1.0, 3.0, 10.0],\n",
    "    'lr__penalty': ['l2'],\n",
    "    'lr__solver': ['lbfgs'],\n",
    "}\n",
    "\n",
    "search = GridSearchCV(pipe, param_grid=param_grid, scoring='roc_auc', cv=5, n_jobs=None)\n",
    "search.fit(X_train, y_train)\n",
    "print('Best CV AUC:', round(search.best_score_, 4))\n",
    "print('Best params:', search.best_params_)\n",
    "\n",
    "# Single final evaluation on the held-out test set\n",
    "best_model = search.best_estimator_\n",
    "test_auc = roc_auc_score(y_test, best_model.predict_proba(X_test)[:, 1])\n",
    "print('Test AUC:', round(test_auc, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70239d2",
   "metadata": {},
   "source": [
    "## 8) Practical checklist\n",
    "\n",
    "Before trusting an evaluation, verify:\n",
    "\n",
    "### Split design\n",
    "\n",
    "1. Prediction timestamp/event is defined.\n",
    "2. Split matches deployment:\n",
    "   - forward-in-time prediction → temporal split / rolling CV\n",
    "   - new entities → group split\n",
    "   - panel forecasting → within-entity time split (and consider a time gap)\n",
    "\n",
    "### Leakage defenses\n",
    "\n",
    "3. No preprocessing leakage: use pipelines.\n",
    "4. No target leakage: exclude post-outcome variables and proxies.\n",
    "5. No validation leakage: test set is evaluated once.\n",
    "\n",
    "### Forensics\n",
    "\n",
    "6. Entity overlap counts are zero when they should be.\n",
    "7. Near-duplicate overlap is low.\n",
    "8. Performance stability across folds is acceptable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8273770d",
   "metadata": {},
   "source": [
    "## 9) Exercises\n",
    "\n",
    "1. In the complaints dataset, create a time split at 70/30 and compare with 80/20 and 90/10.\n",
    "2. In listings, change the task to `availability_365 > 200` and compare random vs host-disjoint splits.\n",
    "3. In the panel data (`states_all.csv`), try predicting `AVG_READING_8_SCORE` instead of math.\n",
    "4. Add a time gap to the panel split (e.g., skip one year between train and test) and observe the impact.\n",
    "5. For the complaints dataset, identify at least three columns that are post-outcome and should not be used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19340299",
   "metadata": {},
   "source": [
    "### Key takeaway\n",
    "\n",
    "Split design is part of modeling. If you want reliable performance, you must evaluate under the same dependency structure and information constraints that exist in production.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
