{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f14c8d9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>@import url('https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap');\n",
       "@import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;700&display=swap');\n",
       "@import url('https://fonts.googleapis.com/css2?family=Source+Sans+3:wght@400;700&display=swap');\n",
       "@import url('https://fonts.googleapis.com/css2?family=Merriweather:wght@400;700&display=swap');\n",
       "\n",
       "\n",
       "/* Text areas (Classic Notebook + JupyterLab) */\n",
       ".rendered_html, .text_cell_render, .jp-RenderedHTMLCommon,\n",
       "div.rendered_html, .output_subarea {\n",
       "  font-family: \"Roboto\", \"Inter\", \"Source Sans 3\", system-ui, -apple-system, \"Segoe UI\", Arial, sans-serif; !important;\n",
       "}\n",
       "\n",
       "/* RTL for rendered markdown/html content */\n",
       ".rendered_html, .text_cell_render, .jp-RenderedHTMLCommon,\n",
       "h1, h2, h3, h4, h5, h6, p, strong, label, ol, ul, li {\n",
       "  line-height: 1.9;\n",
       "  font-family: \"Roboto\", \"Inter\", \"Source Sans 3\", system-ui, -apple-system, \"Segoe UI\", Arial, sans-serif; !important;\n",
       "}\n",
       "\n",
       "h1 { font-size: 2rem; color: #070F2B; font-weight: 900}\n",
       "h2 { font-size: 1.8rem; color: #070F2B; font-weight: 800}\n",
       "h3 { font-size: 1.6rem; color: #070F2B; font-weight: 700}\n",
       "h4 { font-size: 1.4rem; color: #1E3E62; font-weight: 600}\n",
       "h5 { font-size: 1.2rem; color: #1E3E62}\n",
       "h6 { font-size: 1rem; color: #1E3E62}\n",
       "p { font-size: 1.1rem; ; font-weight: 400}\n",
       "ol, ul, li, th, td  { font-size: 1rem; }\n",
       "\n",
       "/* Keep code + outputs LTR and monospace */\n",
       ".CodeMirror, .CodeMirror *,\n",
       "pre, code,\n",
       ".jp-CodeCell .jp-InputArea, .jp-CodeCell .jp-InputArea *,\n",
       ".jp-OutputArea, .jp-OutputArea * {\n",
       "  direction: ltr !important;\n",
       "  text-align: left !important;\n",
       "  font-size: 1rem !important;\n",
       "  font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, \"Liberation Mono\", monospace !important;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from IPython.display import HTML, display\n",
    "css = Path(\"../../../css/custom.css\").read_text(encoding=\"utf-8\")\n",
    "display(HTML(f\"<style>{css}</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d16843",
   "metadata": {},
   "source": [
    "# Chapter 1 — Introduction to Machine Learning\n",
    "# Lesson 10: Learning Paradigms and Task Taxonomy (Regression, Classification, Ranking, Forecasting)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f570845",
   "metadata": {},
   "source": [
    "## Chapter 1 — Introduction to Machine Learning\n",
    "## Lesson 10: Learning Paradigms and Task Taxonomy (Regression, Classification, Ranking, Forecasting)\n",
    "\n",
    "\n",
    "This notebook is part of a bilingual, notebook-first machine learning course.\n",
    "\n",
    "### Why this lesson exists\n",
    "\n",
    "Practitioners frequently start with algorithms (\"Should we use XGBoost?\") before clarifying the **task**.\n",
    "That order is backwards. A clean task definition determines:\n",
    "\n",
    "- what the target is ($y$),\n",
    "- what data generation assumptions you can reasonably make (IID vs time-dependent),\n",
    "- which metrics you will report,\n",
    "- and what validation protocol is legitimate.\n",
    "\n",
    "### Scope\n",
    "\n",
    "We focus on the most common task families in classical ML:\n",
    "\n",
    "1. Regression\n",
    "2. Classification (binary + multi-class)\n",
    "3. Ranking / Learning-to-Rank\n",
    "4. Forecasting (supervised learning under temporal dependence)\n",
    "\n",
    "We will also connect these to **learning paradigms** (supervised/unsupervised/semi-supervised/RL),\n",
    "but the bulk of the hands-on work is supervised.\n",
    "\n",
    "---\n",
    "\n",
    "## Learning objectives\n",
    "\n",
    "By the end of this lesson, you should be able to:\n",
    "\n",
    "1. Map a business question to a *task type* (regression/classification/ranking/forecasting).\n",
    "2. Choose appropriate **losses** and **metrics** for each task.\n",
    "3. Recognize the differences between **IID supervised learning** and **time-dependent** forecasting evaluation.\n",
    "4. Build minimal, correct baselines in scikit-learn and interpret results.\n",
    "5. Explain common failure modes (metric mismatch, data leakage, invalid splits).\n",
    "\n",
    "---\n",
    "\n",
    "## A mental model: \"Task = target type + evaluation protocol + decision\"\n",
    "\n",
    "A practical definition of a task is:\n",
    "\n",
    "$$\n",
    "\\text{Task} = \\big(\\text{target type}, \\; \\text{valid evaluation}, \\; \\text{decision cost}\\big)\n",
    "$$\n",
    "\n",
    "If you only define the target type and ignore evaluation, you'll likely ship a model that fails in production.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd6e2233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete. Versions:\n",
      "  pandas: 2.2.3\n",
      "  numpy: 2.1.2\n",
      "  sklearn: 1.5.2\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error, mean_squared_error, r2_score,\n",
    "    accuracy_score, f1_score, classification_report,\n",
    "    confusion_matrix, roc_auc_score\n",
    ")\n",
    "from sklearn.linear_model import LinearRegression, Ridge, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "def read_csv_or_sample(path, sample_csv_text, **kwargs):\n",
    "    \"\"\"Read a CSV from repo path; fall back to embedded sample rows if missing.\"\"\"\n",
    "    p = Path(path)\n",
    "    if p.exists():\n",
    "        return pd.read_csv(p, **kwargs)\n",
    "    return pd.read_csv(io.StringIO(sample_csv_text), **kwargs)\n",
    "\n",
    "def synthesize_from_sample(df, n=500, noise=0.05, seed=42):\n",
    "    \"\"\"Bootstrap + jitter to make a larger dataset for demonstrative modeling.\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    df_big = df.sample(n=n, replace=True, random_state=seed).reset_index(drop=True)\n",
    "    for col in df_big.columns:\n",
    "        if pd.api.types.is_numeric_dtype(df_big[col]):\n",
    "            std = df_big[col].std(ddof=0)\n",
    "            if np.isfinite(std) and std > 0:\n",
    "                df_big[col] = df_big[col] + rng.normal(0, noise*std, size=len(df_big))\n",
    "    return df_big\n",
    "\n",
    "def synthesize_earthquake_timeseries(df_small, days=220, avg_events_per_day=4, seed=19):\n",
    "    \"\"\"Create a multi-day series when only sample rows are available.\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    df_small = df_small.copy()\n",
    "    df_small[\"timestamp\"] = pd.to_datetime(df_small[\"date\"].astype(str) + \" \" + df_small[\"time\"].astype(str), errors=\"coerce\")\n",
    "    df_small = df_small.dropna(subset=[\"timestamp\"])\n",
    "\n",
    "    start = df_small[\"timestamp\"].min().normalize()\n",
    "    rows = []\n",
    "    for d in range(days):\n",
    "        day = start + pd.Timedelta(days=d)\n",
    "        m = rng.poisson(avg_events_per_day) + 1\n",
    "        sample = df_small.sample(n=m, replace=True, random_state=int(seed + d)).reset_index(drop=True)\n",
    "        secs = rng.integers(0, 24*3600, size=m)\n",
    "        sample[\"timestamp\"] = day + pd.to_timedelta(secs, unit=\"s\")\n",
    "\n",
    "        for col in [\"latitude\", \"longitude\", \"depth\", \"magnitude\"]:\n",
    "            if col in sample.columns:\n",
    "                sample[col] = pd.to_numeric(sample[col], errors=\"coerce\")\n",
    "                std = np.nanstd(sample[col])\n",
    "                if np.isfinite(std) and std > 0:\n",
    "                    sample[col] = sample[col] + rng.normal(0, 0.10*std, size=m)\n",
    "\n",
    "        rows.append(sample)\n",
    "\n",
    "    out = pd.concat(rows, ignore_index=True)\n",
    "    out[\"date\"] = out[\"timestamp\"].dt.date.astype(str)\n",
    "    out[\"time\"] = out[\"timestamp\"].dt.time.astype(str)\n",
    "    return out.drop(columns=[\"timestamp\"])\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    \"\"\"Compute RMSE without using deprecated sklearn squared=... parameter.\"\"\"\n",
    "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "print(\"Setup complete. Versions:\")\n",
    "import sklearn\n",
    "print(\"  pandas:\", pd.__version__)\n",
    "print(\"  numpy:\", np.__version__)\n",
    "print(\"  sklearn:\", sklearn.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30ea329",
   "metadata": {},
   "source": [
    "## 1. Learning paradigms: where supervision comes from\n",
    "\n",
    "A **learning paradigm** specifies how the model obtains information that defines \"good behavior\".\n",
    "\n",
    "### 1.1 Supervised learning\n",
    "\n",
    "You observe labeled pairs $(x_i, y_i)$ and learn a mapping $f: x \\mapsto y$.\n",
    "\n",
    "Typical supervised objective:\n",
    "\n",
    "$$\n",
    "\\hat{f} = \\arg\\min_{f \\in \\mathcal{F}} \\; \\frac{1}{n} \\sum_{i=1}^n \\ell\\big(y_i, f(x_i)\\big) + \\lambda \\Omega(f)\n",
    "$$\n",
    "\n",
    "- $\\ell$ is a *loss* (how wrong a prediction is)\n",
    "- $\\Omega$ is a regularizer (capacity control)\n",
    "- $\\lambda$ tunes the regularization strength\n",
    "\n",
    "### 1.2 Unsupervised learning\n",
    "\n",
    "You observe $x_i$ without explicit targets and learn structure: clusters, densities, embeddings.\n",
    "Unsupervised learning is frequently used for feature learning, segmentation, and anomaly detection.\n",
    "\n",
    "### 1.3 Semi-supervised and weak supervision\n",
    "\n",
    "You have a small labeled set and a large unlabeled set.\n",
    "You may use pseudo-labels or graph structure to propagate labels.\n",
    "In practice, a \"weak supervision\" system can generate noisy labels from heuristic rules.\n",
    "\n",
    "### 1.4 Reinforcement learning (RL) and bandits\n",
    "\n",
    "You interact with an environment, choose actions, and receive rewards.\n",
    "This is not the focus here, but note that some ranking/recommendation problems are treated as contextual bandits.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Task taxonomy: target type and evaluation protocol\n",
    "\n",
    "A compact taxonomy for common supervised tasks:\n",
    "\n",
    "| Task type | Target $y$ | Prediction output | Typical metric |\n",
    "|---|---|---|---|\n",
    "| Regression | $\\mathbb{R}$ | value | MAE, RMSE, $R^2$ |\n",
    "| Binary classification | $\\{0,1\\}$ | label/probability | F1, ROC AUC, PR AUC |\n",
    "| Multi-class classification | $\\{1,\\dots,K\\}$ | class probs | accuracy, macro-F1 |\n",
    "| Ranking (L2R) | relevance / pairwise prefs | ordering | NDCG@k, MAP, Recall@k |\n",
    "| Forecasting | $y_{t+h}$ | future value | MAE/RMSE by horizon |\n",
    "\n",
    "Notice: **forecasting** is usually supervised but **not IID**.\n",
    "That changes what \"train/test split\" means.\n",
    "\n",
    "---\n",
    "\n",
    "## 2.1 Cost, thresholds, and decision rules\n",
    "\n",
    "Even for the same task type, costs can change the decision rule.\n",
    "\n",
    "For binary classification, a common decision is:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\mathbf{1}\\big[\\; P(y=1\\mid x) \\ge \\tau \\;\\big]\n",
    "$$\n",
    "\n",
    "The threshold $\\tau$ should reflect false positive vs false negative costs,\n",
    "not a default $0.5$.\n",
    "\n",
    "For ranking, you often care about the *top* of the list because of limited bandwidth:\n",
    "\n",
    "- You email the top 1% of leads\n",
    "- You show the top 10 products\n",
    "- You review the top 50 flagged transactions\n",
    "\n",
    "So you should evaluate with top-$k$ metrics.\n",
    "\n",
    "---\n",
    "\n",
    "## 2.2 A small checklist for task framing\n",
    "\n",
    "Before training anything, write down:\n",
    "\n",
    "1. What is the target $y$ (including units)?\n",
    "2. What action follows the prediction?\n",
    "3. What is the cost of false positives/negatives or large errors?\n",
    "4. What data splitting rule matches production?\n",
    "\n",
    "We'll now make these ideas concrete with code examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216fc219",
   "metadata": {},
   "source": [
    "## 3. Toy framing: regression vs classification vs ranking\n",
    "\n",
    "This toy example uses a single feature $x$ and an underlying continuous outcome.\n",
    "\n",
    "- In **regression**, you predict $y$ directly.\n",
    "- In **classification**, you predict whether $y$ exceeds a threshold.\n",
    "- In **ranking**, you might use the predicted score to order instances by risk.\n",
    "\n",
    "Even though they use the same raw data, the learned parameters and the evaluation are different.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03ecb42f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y_cont</th>\n",
       "      <th>y_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.125730</td>\n",
       "      <td>-0.911055</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.132105</td>\n",
       "      <td>-0.373606</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.640423</td>\n",
       "      <td>0.657890</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.104900</td>\n",
       "      <td>-0.156333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.535669</td>\n",
       "      <td>-1.343468</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.361595</td>\n",
       "      <td>0.565040</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.304000</td>\n",
       "      <td>2.813815</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.947081</td>\n",
       "      <td>2.415419</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.703735</td>\n",
       "      <td>-1.471738</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1.265421</td>\n",
       "      <td>-1.847611</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.623274</td>\n",
       "      <td>-1.579146</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.041326</td>\n",
       "      <td>0.258407</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           x    y_cont  y_bin\n",
       "0   0.125730 -0.911055      0\n",
       "1  -0.132105 -0.373606      0\n",
       "2   0.640423  0.657890      1\n",
       "3   0.104900 -0.156333      1\n",
       "4  -0.535669 -1.343468      0\n",
       "5   0.361595  0.565040      1\n",
       "6   1.304000  2.813815      1\n",
       "7   0.947081  2.415419      1\n",
       "8  -0.703735 -1.471738      0\n",
       "9  -1.265421 -1.847611      0\n",
       "10 -0.623274 -1.579146      0\n",
       "11  0.041326  0.258407      1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression coefficients: 1.9512520123535053 intercept: -0.12409791654978095\n",
      "Classification coefficient: 1.4488474737404589 intercept: -0.027935787302401996\n",
      "\n",
      "First 5 predictions:\n",
      "  x= 0.126  y_cont=-0.911  y_hat_reg= 0.121  p(y=1)= 0.538\n",
      "  x=-0.132  y_cont=-0.374  y_hat_reg=-0.382  p(y=1)= 0.445\n",
      "  x= 0.640  y_cont= 0.658  y_hat_reg= 1.126  p(y=1)= 0.711\n",
      "  x= 0.105  y_cont=-0.156  y_hat_reg= 0.081  p(y=1)= 0.531\n",
      "  x=-0.536  y_cont=-1.343  y_hat_reg=-1.169  p(y=1)= 0.309\n"
     ]
    }
   ],
   "source": [
    "# A tiny illustration: the same data can be framed as regression or classification.\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "n = 12\n",
    "x = rng.normal(size=n)\n",
    "y_cont = 2*x + rng.normal(scale=0.5, size=n)          # continuous target\n",
    "y_bin = (y_cont > np.median(y_cont)).astype(int)      # binarized target\n",
    "\n",
    "toy = pd.DataFrame({\"x\": x, \"y_cont\": y_cont, \"y_bin\": y_bin})\n",
    "display(toy)\n",
    "\n",
    "# Regression framing\n",
    "reg = LinearRegression().fit(toy[[\"x\"]], toy[\"y_cont\"])\n",
    "y_hat_reg = reg.predict(toy[[\"x\"]])\n",
    "\n",
    "# Classification framing\n",
    "clf = LogisticRegression().fit(toy[[\"x\"]], toy[\"y_bin\"])\n",
    "p_hat = clf.predict_proba(toy[[\"x\"]])[:, 1]\n",
    "\n",
    "print(\"Regression coefficients:\", reg.coef_[0], \"intercept:\", reg.intercept_)\n",
    "print(\"Classification coefficient:\", clf.coef_[0,0], \"intercept:\", clf.intercept_[0])\n",
    "print(\"\\nFirst 5 predictions:\")\n",
    "for i in range(5):\n",
    "    print(f\"  x={x[i]: .3f}  y_cont={y_cont[i]: .3f}  y_hat_reg={y_hat_reg[i]: .3f}  p(y=1)={p_hat[i]: .3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d156329",
   "metadata": {},
   "source": [
    "## 4. Regression example: house price prediction\n",
    "\n",
    "### 4.1 Problem statement\n",
    "\n",
    "Given features about a house, predict its selling price:\n",
    "\n",
    "- Inputs $x$: size, bedrooms, bathrooms, neighborhood, etc.\n",
    "- Target $y$: price (currency)\n",
    "\n",
    "This is a regression task because $y \\in \\mathbb{R}$.\n",
    "\n",
    "### 4.2 Baseline modeling approach\n",
    "\n",
    "We'll build:\n",
    "\n",
    "1. A preprocessing pipeline:\n",
    "   - numeric: median imputation + standardization\n",
    "   - categorical: most-frequent imputation + one-hot encoding\n",
    "2. A linear regression model\n",
    "\n",
    "### 4.3 Metrics\n",
    "\n",
    "We report MAE, RMSE, and $R^2$:\n",
    "\n",
    "- MAE is interpretable in the target unit.\n",
    "- RMSE penalizes large errors more.\n",
    "- $R^2$ is useful for variance explained but can be misleading across datasets.\n",
    "\n",
    "Let's implement the baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fcf03be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw rows (head):\n",
      " Home  Price  SqFt  Bedrooms  Bathrooms  Offers Brick Neighborhood\n",
      "    1 114300  1790         2          2       2    No         East\n",
      "    2 114200  2030         4          2       3    No         East\n",
      "    3 114800  1740         3          2       1    No         East\n",
      "    4  94700  1980         3          2       3    No         East\n",
      "    5 119800  2130         3          3       3    No         East\n",
      "MAE:  7,283.09\n",
      "RMSE: 9,529.77\n",
      "R^2:  0.889\n"
     ]
    }
   ],
   "source": [
    "# Regression dataset: house-prices\n",
    "HOUSE_PATH = \"../../../Datasets/Regression/house-prices.csv\"\n",
    "sample_house = \"Home,Price,SqFt,Bedrooms,Bathrooms,Offers,Brick,Neighborhood\\n1,114300,1790,2,2,2,No,East\\n2,114200,2030,4,2,3,No,East\\n3,114800,1740,3,2,1,No,East\\n4,94700,1980,3,2,3,No,East\\n5,119800,2130,3,3,3,No,East\\n\"\n",
    "\n",
    "house_df = read_csv_or_sample(HOUSE_PATH, sample_house)\n",
    "print(\"Raw rows (head):\")\n",
    "print(house_df.head().to_string(index=False))\n",
    "\n",
    "df = synthesize_from_sample(house_df, n=500, noise=0.08, seed=13)\n",
    "\n",
    "y = df[\"Price\"].astype(float)\n",
    "X = df.drop(columns=[\"Price\"]).copy()\n",
    "\n",
    "num_cols = [\"SqFt\", \"Bedrooms\", \"Bathrooms\", \"Offers\"]\n",
    "cat_cols = [\"Brick\", \"Neighborhood\"]\n",
    "\n",
    "pre = ColumnTransformer([\n",
    "    (\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "                      (\"sc\", StandardScaler())]), num_cols),\n",
    "    (\"cat\", Pipeline([(\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                      (\"oh\", OneHotEncoder(handle_unknown=\"ignore\"))]), cat_cols),\n",
    "])\n",
    "\n",
    "model = Pipeline([(\"pre\", pre), (\"linreg\", LinearRegression())])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, pred)\n",
    "rmse_val = rmse(y_test, pred)\n",
    "r2 = r2_score(y_test, pred)\n",
    "\n",
    "print(f\"MAE:  {mae:,.2f}\")\n",
    "print(f\"RMSE: {rmse_val:,.2f}\")\n",
    "print(f\"R^2:  {r2:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602c3382",
   "metadata": {},
   "source": [
    "## 5. Binary classification example: diabetes prediction\n",
    "\n",
    "### 5.1 Problem statement\n",
    "\n",
    "Given patient measurements, predict whether the patient is diabetic.\n",
    "\n",
    "- Inputs $x$: glucose, BMI, age, etc.\n",
    "- Target $y \\in \\{0,1\\}$: diabetic vs non-diabetic\n",
    "\n",
    "### 5.2 Probabilistic prediction matters\n",
    "\n",
    "Many downstream decisions depend on *risk* rather than a hard label.\n",
    "Logistic regression returns an estimated probability $P(y=1\\mid x)$.\n",
    "\n",
    "### 5.3 Metrics\n",
    "\n",
    "We'll compute:\n",
    "\n",
    "- Accuracy\n",
    "- F1\n",
    "- ROC AUC\n",
    "- Confusion matrix\n",
    "\n",
    "In imbalanced datasets, accuracy alone can be misleading; F1 and AUC provide additional signal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "261ac062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw rows (head):\n",
      " Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin  BMI  DiabetesPedigreeFunction  Age classification\n",
      "           6      148             72             35        0 33.6                     0.627   50       Diabetic\n",
      "           1       85             66             29        0 26.6                     0.351   31   Non-Diabetic\n",
      "           8      183             64              0        0 23.3                     0.672   32       Diabetic\n",
      "           1       89             66             23       94 28.1                     0.167   21   Non-Diabetic\n",
      "           0      137             40             35      168 43.1                     2.288   33       Diabetic\n",
      "Accuracy: 0.793\n",
      "F1:       0.667\n",
      "ROC AUC:  0.828\n",
      "Confusion matrix [[TN, FP], [FN, TP]]:\n",
      "[[88  8]\n",
      " [23 31]]\n"
     ]
    }
   ],
   "source": [
    "# Classification dataset: diabetes\n",
    "DIAB_PATH = \"../../../Datasets/Classification/diabetes.csv\"\n",
    "sample_diabetes = \"Pregnancies,Glucose,BloodPressure,SkinThickness,Insulin,BMI,DiabetesPedigreeFunction,Age,classification\\n6,148,72,35,0,33.6,0.627,50,Diabetic\\n1,85,66,29,0,26.6,0.351,31,Non-Diabetic\\n8,183,64,0,0,23.3,0.672,32,Diabetic\\n1,89,66,23,94,28.1,0.167,21,Non-Diabetic\\n0,137,40,35,168,43.1,2.288,33,Diabetic\\n\"\n",
    "\n",
    "diab_df = read_csv_or_sample(DIAB_PATH, sample_diabetes)\n",
    "print(\"Raw rows (head):\")\n",
    "print(diab_df.head().to_string(index=False))\n",
    "\n",
    "df = synthesize_from_sample(diab_df, n=600, noise=0.10, seed=7)\n",
    "\n",
    "y = (df[\"classification\"].astype(str) == \"Diabetic\").astype(int)\n",
    "X = df.drop(columns=[\"classification\"])\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"sc\", StandardScaler()),\n",
    "    (\"logreg\", LogisticRegression(max_iter=1000, random_state=0)),\n",
    "])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0, stratify=y)\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "proba = pipe.predict_proba(X_test)[:, 1]\n",
    "pred = (proba >= 0.5).astype(int)\n",
    "\n",
    "acc = accuracy_score(y_test, pred)\n",
    "f1 = f1_score(y_test, pred)\n",
    "auc = roc_auc_score(y_test, proba)\n",
    "cm = confusion_matrix(y_test, pred)\n",
    "\n",
    "print(f\"Accuracy: {acc:.3f}\")\n",
    "print(f\"F1:       {f1:.3f}\")\n",
    "print(f\"ROC AUC:  {auc:.3f}\")\n",
    "print(\"Confusion matrix [[TN, FP], [FN, TP]]:\")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0e15ff",
   "metadata": {},
   "source": [
    "## 6. Multi-class classification: Iris\n",
    "\n",
    "Multi-class classification predicts among $K>2$ classes.\n",
    "Even when accuracy looks high, always inspect per-class performance with a classification report.\n",
    "\n",
    "We use the iris dataset for a compact demonstration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6d9fc81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw rows (head):\n",
      " sepal_length  sepal_width  petal_length  petal_width classification\n",
      "          5.4          3.7           1.5          0.2    Iris-setosa\n",
      "          4.8          3.4           1.6          0.2    Iris-setosa\n",
      "          4.8          3.0           1.4          0.1    Iris-setosa\n",
      "          4.3          3.0           1.1          0.1    Iris-setosa\n",
      "          5.8          4.0           1.2          0.2    Iris-setosa\n",
      "Accuracy (multi-class): 0.948\n",
      "\n",
      "Classification report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    Iris-setosa       1.00      1.00      1.00        42\n",
      "Iris-versicolor       0.97      0.86      0.92        44\n",
      " Iris-virginica       0.89      0.98      0.93        49\n",
      "\n",
      "       accuracy                           0.95       135\n",
      "      macro avg       0.95      0.95      0.95       135\n",
      "   weighted avg       0.95      0.95      0.95       135\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Multi-class dataset: iris\n",
    "IRIS_PATH = \"../../../Datasets/Classification/iris.csv\"\n",
    "sample_iris = \"sepal_length,sepal_width,petal_length,petal_width,classification\\n5.1,3.5,1.4,0.2,Iris-setosa\\n4.9,3.0,1.4,0.2,Iris-setosa\\n5.0,3.6,1.4,0.2,Iris-setosa\\n7.0,3.2,4.7,1.4,Iris-versicolor\\n6.4,3.2,4.5,1.5,Iris-versicolor\\n6.9,3.1,4.9,1.5,Iris-versicolor\\n6.3,3.3,6.0,2.5,Iris-virginica\\n5.8,2.7,5.1,1.9,Iris-virginica\\n7.1,3.0,5.9,2.1,Iris-virginica\\n\"\n",
    "\n",
    "iris_df = read_csv_or_sample(IRIS_PATH, sample_iris)\n",
    "print(\"Raw rows (head):\")\n",
    "print(iris_df.head().to_string(index=False))\n",
    "\n",
    "df = synthesize_from_sample(iris_df, n=450, noise=0.08, seed=11)\n",
    "\n",
    "y = df[\"classification\"].astype(str)\n",
    "X = df.drop(columns=[\"classification\"])\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"sc\", StandardScaler()),\n",
    "    (\"logreg\", LogisticRegression(max_iter=1000, random_state=0)),\n",
    "])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0, stratify=y)\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "pred = pipe.predict(X_test)\n",
    "\n",
    "acc = accuracy_score(y_test, pred)\n",
    "print(f\"Accuracy (multi-class): {acc:.3f}\")\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_test, pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73c3f5e",
   "metadata": {},
   "source": [
    "## 7. Ranking / Learning-to-Rank baseline\n",
    "\n",
    "### 7.1 What makes ranking different?\n",
    "\n",
    "In classification, each example is independent and has a label.\n",
    "\n",
    "In ranking, you typically have *queries* (users, sessions, contexts) and a set of candidate items.\n",
    "You must order items to maximize utility at the top of the list.\n",
    "\n",
    "### 7.2 Pointwise vs pairwise vs listwise\n",
    "\n",
    "- **Pointwise:** predict a relevance score per item, then sort\n",
    "- **Pairwise:** learn preferences between pairs (A should rank above B)\n",
    "- **Listwise:** optimize a list-level objective directly\n",
    "\n",
    "We implement a pointwise baseline because it is simple and widely used as a first step.\n",
    "\n",
    "### 7.3 Evaluation with NDCG\n",
    "\n",
    "NDCG emphasizes the beginning of the ranked list:\n",
    "\n",
    "$$\n",
    "\\mathrm{DCG}@k = \\sum_{i=1}^{k} \\frac{2^{rel_i}-1}{\\log_2(i+1)}, \\quad\n",
    "\\mathrm{NDCG}@k = \\frac{\\mathrm{DCG}@k}{\\mathrm{IDCG}@k}\n",
    "$$\n",
    "\n",
    "We'll create a synthetic relevance label from available features and compute NDCG@10 and NDCG@50.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "112d149c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw rows (head):\n",
      "   id                                             name  host_id host_name  neighbourhood_group          neighbourhood  latitude  longitude       room_type  price  minimum_nights  number_of_reviews last_review  reviews_per_month  calculated_host_listings_count  availability_365  number_of_reviews_ltm  license\n",
      "13913              Holiday London DB Room Let-on going    54730     Alina                  NaN              Islington  51.56861   -0.11270    Private room   57.0               1                 51  2025-02-09               0.29                               3               344                     10      NaN\n",
      "15400              Bright Chelsea  Apartment. Chelsea!    60302  Philippa                  NaN Kensington and Chelsea  51.48780   -0.16813 Entire home/apt    NaN               4                 96  2024-04-28               0.52                               1                11                      2      NaN\n",
      "17402 Very Central Modern 3-Bed/2 Bath By Oxford St W1    67564       Liz                  NaN            Westminster  51.52195   -0.14094 Entire home/apt  510.0               3                 56  2024-02-19               0.33                               5               293                      0      NaN\n",
      "24328                 Battersea live/work artist house    41759       Joe                  NaN             Wandsworth  51.47072   -0.16266 Entire home/apt  213.0              90                 94  2022-07-19               0.54                               1               194                      0      NaN\n",
      "31036   Bright  compact 1 Bedroom Apartment Brick Lane   133271  Hendryks                  NaN          Tower Hamlets  51.52425   -0.06997 Entire home/apt  100.0               2                126  2025-02-20               0.70                               8               353                      3      NaN\n",
      "NDCG@10: 1.0\n",
      "NDCG@50: 1.0\n",
      "\n",
      "Top-10 ranked items (sample):\n",
      "                                              name neighbourhood       room_type      price  number_of_reviews  availability_365  relevance\n",
      "                    Contemporary 1 bed in Peckham.     Southwark Entire home/apt  88.176443          24.736264        211.228017          3\n",
      "              Cozy large double bedroom in Clapham       Lambeth    Private room  49.250978         154.549350        148.867355          3\n",
      " Charming designer 1 bed on Charlie Chaplin Street       Lambeth Entire home/apt  87.440453          59.878215        171.795480          3\n",
      "                        Modern Studio in Fitzrovia        Camden Entire home/apt  98.876716          36.658148        129.114251          3\n",
      "                                   Women only room Tower Hamlets    Private room  62.625605          22.942540        254.631726          3\n",
      "Modern New 1 Bedroom Flat Oval/Kennington Flat No2       Lambeth Entire home/apt 107.623338         114.725058        160.265335          3\n",
      "           Charming double studio with kitchenette         Brent Entire home/apt  98.039241          24.212722        371.669066          3\n",
      "     Double bedroom in central London flat, Zone 2     Southwark    Private room  75.518620          23.645129        156.163889          3\n",
      "  Large bedroom with garden 20 minutes to Victoria       Lambeth    Private room  39.003440          31.967961        242.990954          3\n",
      " Beautifully Decorated, Spacious 2 Bed in Brixton!       Lambeth Entire home/apt  93.331393         117.557026        105.565523          3\n"
     ]
    }
   ],
   "source": [
    "# Ranking dataset: listings (Airbnb-style)\n",
    "LIST_PATH = \"../../../Datasets/Regression/listings.csv\"\n",
    "sample_listings = \"id,name,host_id,host_name,neighbourhood_group,neighbourhood,latitude,longitude,room_type,price,minimum_nights,number_of_reviews,last_review,reviews_per_month,calculated_host_listings_count,availability_365,number_of_reviews_ltm,license\\n13913,Holiday London DB Room Let-on going,54730,Alina,,Islington,51.56861,-0.1127,Private room,57,1,51,2025-02-09,0.29,3,344,10,\\n15400,Bright Chelsea  Apartment. Chelsea!,60302,Philippa,,Kensington and Chelsea,51.4878,-0.16813,Entire home/apt,180,4,96,2024-04-28,0.52,1,11,2,\\n17402,Very Central Modern 3-Bed/2 Bath By Oxford St W1,67564,Liz,,Westminster,51.52195,-0.14094,Entire home/apt,510,3,56,2024-02-19,0.33,5,293,0,\\n24328,Battersea live/work artist house,41759,Joe,,Wandsworth,51.47072,-0.16266,Entire home/apt,213,90,94,2022-07-19,0.54,1,194,0,\\n31036,Bright  compact 1 Bedroom Apartment Brick Lane,133271,Hendryks,,Tower Hamlets,51.52425,-0.06997,Entire home/apt,100,2,126,2025-02-20,0.70,8,353,3,\\n\"\n",
    "\n",
    "list_df = read_csv_or_sample(LIST_PATH, sample_listings)\n",
    "print(\"Raw rows (head):\")\n",
    "print(list_df.head().to_string(index=False))\n",
    "\n",
    "df = synthesize_from_sample(list_df, n=800, noise=0.06, seed=17)\n",
    "\n",
    "# Convert and clip numeric columns to keep them in reasonable ranges (prevents invalid log1p)\n",
    "num_cols_all = [\"price\", \"minimum_nights\", \"number_of_reviews\", \"reviews_per_month\",\n",
    "                \"calculated_host_listings_count\", \"availability_365\"]\n",
    "for c in num_cols_all:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "df[\"price\"] = df[\"price\"].fillna(df[\"price\"].median()).clip(lower=0)\n",
    "df[\"reviews_per_month\"] = df[\"reviews_per_month\"].fillna(0.0).clip(lower=0)\n",
    "df[\"number_of_reviews\"] = df[\"number_of_reviews\"].fillna(0.0).clip(lower=0)\n",
    "df[\"availability_365\"] = df[\"availability_365\"].fillna(df[\"availability_365\"].median()).clip(lower=0)\n",
    "df[\"minimum_nights\"] = df[\"minimum_nights\"].fillna(df[\"minimum_nights\"].median()).clip(lower=1)\n",
    "df[\"calculated_host_listings_count\"] = df[\"calculated_host_listings_count\"].fillna(1.0).clip(lower=1)\n",
    "\n",
    "df[\"room_type\"] = df[\"room_type\"].astype(str)\n",
    "df[\"neighbourhood\"] = df[\"neighbourhood\"].astype(str)\n",
    "\n",
    "# Synthetic relevance label (for demonstration only)\n",
    "score = (0.6*np.log1p(df[\"number_of_reviews\"]) + 0.3*np.log1p(df[\"availability_365\"]) - 0.002*df[\"price\"])\n",
    "qs = score.quantile([0.25, 0.5, 0.75]).values\n",
    "df[\"relevance\"] = np.digitize(score, qs).astype(int)\n",
    "\n",
    "def ndcg_at_k(y_true, y_score, k=10):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_score = np.asarray(y_score)\n",
    "    order = np.argsort(y_score)[::-1][:k]\n",
    "    gains = (2**y_true[order] - 1)\n",
    "    discounts = 1.0 / np.log2(np.arange(2, len(order)+2))\n",
    "    dcg = np.sum(gains * discounts)\n",
    "\n",
    "    ideal_order = np.argsort(y_true)[::-1][:k]\n",
    "    ideal_gains = (2**y_true[ideal_order] - 1)\n",
    "    idcg = np.sum(ideal_gains * discounts)\n",
    "    return float(dcg / idcg) if idcg > 0 else 0.0\n",
    "\n",
    "y = df[\"relevance\"].astype(int)\n",
    "X = df[[\n",
    "    \"price\", \"minimum_nights\", \"number_of_reviews\", \"reviews_per_month\",\n",
    "    \"calculated_host_listings_count\", \"availability_365\",\n",
    "    \"room_type\", \"neighbourhood\"\n",
    "]].copy()\n",
    "\n",
    "num_cols = [\"price\", \"minimum_nights\", \"number_of_reviews\", \"reviews_per_month\",\n",
    "            \"calculated_host_listings_count\", \"availability_365\"]\n",
    "cat_cols = [\"room_type\", \"neighbourhood\"]\n",
    "\n",
    "pre = ColumnTransformer([\n",
    "    (\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "                      (\"sc\", StandardScaler())]), num_cols),\n",
    "    (\"cat\", Pipeline([(\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                      (\"oh\", OneHotEncoder(handle_unknown=\"ignore\"))]), cat_cols)\n",
    "])\n",
    "\n",
    "ranker = Pipeline([\n",
    "    (\"pre\", pre),\n",
    "    (\"rf\", RandomForestRegressor(n_estimators=200, random_state=0, n_jobs=-1, min_samples_leaf=3))\n",
    "])\n",
    "\n",
    "# Hold out entire neighbourhoods (group-aware evaluation)\n",
    "unique_neigh = df[\"neighbourhood\"].unique()\n",
    "rng = np.random.default_rng(0)\n",
    "test_neigh = set(rng.choice(unique_neigh, size=max(1, len(unique_neigh)//4), replace=False))\n",
    "test_mask = df[\"neighbourhood\"].isin(test_neigh)\n",
    "\n",
    "X_train, X_test = X[~test_mask], X[test_mask]\n",
    "y_train, y_test = y[~test_mask], y[test_mask]\n",
    "\n",
    "ranker.fit(X_train, y_train)\n",
    "y_score = ranker.predict(X_test)\n",
    "\n",
    "print(\"NDCG@10:\", round(ndcg_at_k(y_test, y_score, k=10), 3))\n",
    "print(\"NDCG@50:\", round(ndcg_at_k(y_test, y_score, k=50), 3))\n",
    "\n",
    "top_idx = np.argsort(y_score)[::-1][:10]\n",
    "ranked = df.loc[X_test.index[top_idx], [\"name\",\"neighbourhood\",\"room_type\",\"price\",\"number_of_reviews\",\"availability_365\",\"relevance\"]]\n",
    "print(\"\\nTop-10 ranked items (sample):\")\n",
    "print(ranked.reset_index(drop=True).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dc8d7c",
   "metadata": {},
   "source": [
    "## 8. Forecasting example: daily earthquake magnitude\n",
    "\n",
    "### 8.1 Forecasting is supervised, but time-dependent\n",
    "\n",
    "Forecasting predicts $y_{t+h}$ from information up to time $t$.\n",
    "Even though it is supervised, time dependence changes the rules:\n",
    "\n",
    "- You must not train on future data.\n",
    "- You should validate using chronological splits.\n",
    "\n",
    "### 8.2 Feature engineering via lags\n",
    "\n",
    "A simple approach is to aggregate by day and create lag features:\n",
    "\n",
    "- lagged mean magnitude\n",
    "- lagged event counts\n",
    "\n",
    "Then train a regression model.\n",
    "\n",
    "### 8.3 Rolling evaluation\n",
    "\n",
    "We use `TimeSeriesSplit` to emulate a rolling-window evaluation.\n",
    "This is not perfect (real systems may have a gap between train and test), but it's much safer than shuffling.\n",
    "\n",
    "Let's implement it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52d19db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw rows (head):\n",
      "      date     time  latitude  longitude  depth  magnitude\n",
      "2008-11-01 00:31:25     -0.60   98.89553   20.0       2.99\n",
      "2008-11-01 01:34:29     -6.61  129.38722   30.1       5.51\n",
      "2008-11-01 01:38:14     -3.65  127.99068    5.0       3.54\n",
      "2008-11-01 02:20:05     -4.20  128.09700    5.0       2.42\n",
      "2008-11-01 02:32:18     -4.09  128.20047   10.0       2.41\n",
      "TimeSeries CV MAE (mean): 0.2143\n",
      "TimeSeries CV MAE (std):  0.0214\n",
      "\n",
      "Last 5 days (actual vs predicted mean magnitude):\n",
      "      date  mean_magnitude  pred_next_mean_mag  event_count\n",
      "2022-09-22        3.388400            3.467577           25\n",
      "2022-09-23        3.507105            3.343235           38\n",
      "2022-09-24        3.461333            3.539307           15\n",
      "2022-09-25        3.242609            3.466240           23\n",
      "2022-09-26        3.645238            3.492252           21\n"
     ]
    }
   ],
   "source": [
    "# Forecasting dataset: earthquake\n",
    "EQ_PATH = \"../../../Datasets/Regression/earthquake.csv\"\n",
    "sample_earthquake = \"date,time,latitude,longitude,depth,magnitude\\n2008-11-01,00:31:25,-0.6,98.89553,20.0,2.99\\n2008-11-02,01:34:29,-6.61,129.38722,30.1,5.51\\n2008-11-03,01:38:14,-3.65,127.99068,5.0,3.54\\n2008-11-04,02:20:05,-4.2,128.097,5.0,2.42\\n2008-11-05,02:32:18,-4.09,128.20047,10.0,2.41\\n\"\n",
    "\n",
    "eq_df = read_csv_or_sample(EQ_PATH, sample_earthquake)\n",
    "print(\"Raw rows (head):\")\n",
    "print(eq_df.head().to_string(index=False))\n",
    "\n",
    "# If you only have a few sample rows, create a multi-day synthetic series for demonstration.\n",
    "if len(eq_df) < 100:\n",
    "    df = synthesize_earthquake_timeseries(eq_df, days=220, avg_events_per_day=4, seed=19)\n",
    "else:\n",
    "    df = eq_df.copy()\n",
    "\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"date\"].astype(str) + \" \" + df[\"time\"].astype(str), errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"timestamp\"]).sort_values(\"timestamp\")\n",
    "\n",
    "daily = df.set_index(\"timestamp\").resample(\"D\")[\"magnitude\"].agg([\"mean\", \"count\"]).reset_index()\n",
    "daily.columns = [\"date\", \"mean_magnitude\", \"event_count\"]\n",
    "\n",
    "for lag in [1, 2, 3, 7]:\n",
    "    daily[f\"lag_mean_{lag}\"] = daily[\"mean_magnitude\"].shift(lag)\n",
    "    daily[f\"lag_count_{lag}\"] = daily[\"event_count\"].shift(lag)\n",
    "\n",
    "daily = daily.dropna().reset_index(drop=True)\n",
    "\n",
    "y = daily[\"mean_magnitude\"]\n",
    "X = daily.drop(columns=[\"date\", \"mean_magnitude\"])\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "model = Pipeline([\n",
    "    (\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"sc\", StandardScaler()),\n",
    "    (\"ridge\", Ridge(alpha=1.0, random_state=0)),\n",
    "])\n",
    "\n",
    "maes = []\n",
    "for train_idx, test_idx in tscv.split(X):\n",
    "    model.fit(X.iloc[train_idx], y.iloc[train_idx])\n",
    "    pred = model.predict(X.iloc[test_idx])\n",
    "    maes.append(mean_absolute_error(y.iloc[test_idx], pred))\n",
    "\n",
    "print(\"TimeSeries CV MAE (mean):\", round(float(np.mean(maes)), 4))\n",
    "print(\"TimeSeries CV MAE (std): \", round(float(np.std(maes)), 4))\n",
    "\n",
    "model.fit(X, y)\n",
    "daily[\"pred_next_mean_mag\"] = model.predict(X)\n",
    "\n",
    "print(\"\\nLast 5 days (actual vs predicted mean magnitude):\")\n",
    "print(daily[[\"date\", \"mean_magnitude\", \"pred_next_mean_mag\", \"event_count\"]].tail(5).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819b3763",
   "metadata": {},
   "source": [
    "## 9. Summary and key takeaways\n",
    "\n",
    "1. **Start from the target and decision.**  \n",
    "   Algorithms come later.\n",
    "\n",
    "2. **Metrics are task-dependent.**  \n",
    "   - Regression: MAE/RMSE\n",
    "   - Classification: F1/AUC (and calibration if needed)\n",
    "   - Ranking: NDCG@k and other top-$k$ measures\n",
    "   - Forecasting: horizon-aware errors with chronological splits\n",
    "\n",
    "3. **Validation protocol is part of the task definition.**  \n",
    "   Time series requires time-aware evaluation.\n",
    "\n",
    "4. **Baselines matter.**  \n",
    "   A correct simple baseline can beat a complex model trained and evaluated incorrectly.\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Exercises\n",
    "\n",
    "1. In the regression section, replace the linear model with `RandomForestRegressor` and compare MAE/RMSE.\n",
    "2. In the classification section, tune the threshold $\\tau$ to maximize F1 on a validation split.\n",
    "3. Implement Precision@k for the ranking section and compare it with NDCG@k.\n",
    "4. Try expanding-window splits for forecasting and compare errors.\n",
    "\n",
    "---\n",
    "\n",
    "If you'd like, we can add a short \"mini-project\" at the end of this notebook:\n",
    "given a business scenario, you decide the task type, define $y$, choose the metric,\n",
    "and write the minimum viable modeling pipeline.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
