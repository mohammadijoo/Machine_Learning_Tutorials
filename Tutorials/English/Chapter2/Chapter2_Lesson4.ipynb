{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cc11150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>@import url('https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap');\n",
       "@import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;700&display=swap');\n",
       "@import url('https://fonts.googleapis.com/css2?family=Source+Sans+3:wght@400;700&display=swap');\n",
       "@import url('https://fonts.googleapis.com/css2?family=Merriweather:wght@400;700&display=swap');\n",
       "\n",
       "\n",
       "/* Text areas (Classic Notebook + JupyterLab) */\n",
       ".rendered_html, .text_cell_render, .jp-RenderedHTMLCommon,\n",
       "div.rendered_html, .output_subarea {\n",
       "  font-family: \"Roboto\", \"Inter\", \"Source Sans 3\", system-ui, -apple-system, \"Segoe UI\", Arial, sans-serif; !important;\n",
       "}\n",
       "\n",
       "/* RTL for rendered markdown/html content */\n",
       ".rendered_html, .text_cell_render, .jp-RenderedHTMLCommon,\n",
       "h1, h2, h3, h4, h5, h6, p, strong, label, ol, ul, li {\n",
       "  line-height: 1.9;\n",
       "  font-family: \"Roboto\", \"Inter\", \"Source Sans 3\", system-ui, -apple-system, \"Segoe UI\", Arial, sans-serif; !important;\n",
       "}\n",
       "\n",
       "h1 { font-size: 2rem; color: #070F2B; font-weight: 900}\n",
       "h2 { font-size: 1.8rem; color: #070F2B; font-weight: 800}\n",
       "h3 { font-size: 1.6rem; color: #070F2B; font-weight: 700}\n",
       "h4 { font-size: 1.4rem; color: #1E3E62; font-weight: 600}\n",
       "h5 { font-size: 1.2rem; color: #1E3E62}\n",
       "h6 { font-size: 1rem; color: #1E3E62}\n",
       "p { font-size: 1.1rem; ; font-weight: 400}\n",
       "ol, ul, li, th, td  { font-size: 1rem; }\n",
       "\n",
       "/* Keep code + outputs LTR and monospace */\n",
       ".CodeMirror, .CodeMirror *,\n",
       "pre, code,\n",
       ".jp-CodeCell .jp-InputArea, .jp-CodeCell .jp-InputArea *,\n",
       ".jp-OutputArea, .jp-OutputArea * {\n",
       "  direction: ltr !important;\n",
       "  text-align: left !important;\n",
       "  font-size: 1rem !important;\n",
       "  font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, \"Liberation Mono\", monospace !important;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from IPython.display import HTML, display\n",
    "css = Path(\"../../../css/custom.css\").read_text(encoding=\"utf-8\")\n",
    "display(HTML(f\"<style>{css}</style>\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c230ff",
   "metadata": {},
   "source": [
    "# Chapter 2 — Basics of Data and Preprocessing\n",
    "## Lesson 4: Feature Scaling (Normalization & Standardization)\n",
    "\n",
    "### What you will learn in this lesson\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "- Explain *why* feature scaling matters and *when* it does **not**.\n",
    "- Distinguish **standardization** (z-score scaling) from **normalization** (min–max scaling) and from **vector normalization** (unit-norm per sample).\n",
    "- Select an appropriate scaler for a given algorithm (kNN / SVM / Logistic Regression / PCA / k-means / regularized linear models).\n",
    "- Implement scaling correctly **without data leakage** using `Pipeline` and `ColumnTransformer`.\n",
    "- Diagnose scaling problems: outliers, heavy tails, sparse matrices, and mixed data types.\n",
    "- Treat the scaler as a **hyperparameter** and validate it using cross-validation / grid search.\n",
    "- Communicate scaling choices in a reproducible ML workflow.\n",
    "\n",
    "---\n",
    "\n",
    "### Why this topic is “Band 8–9” relevant (advanced framing)\n",
    "\n",
    "At higher proficiency, “feature scaling” is not a checkbox; it is a *modeling decision* that affects:\n",
    "\n",
    "- **Optimization geometry** (conditioning of the problem; gradient descent step sizes).\n",
    "- **Regularization meaning** (L1/L2 penalties are not comparable across features unless scale is controlled).\n",
    "- **Distance and similarity** (kNN, k-means, kernel methods, cosine distance).\n",
    "- **Numerical stability** (floating-point range; poorly scaled features can create underflow/overflow or ill-conditioned matrices).\n",
    "- **Interpretability and governance** (what does a coefficient *mean* if one feature is in dollars and another is in millimeters?).\n",
    "\n",
    "In this lesson you will learn to treat scaling as part of the modeling pipeline, not an isolated preprocessing trick.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40e8cf3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versions:\n",
      "  python: 3.13.0\n",
      "  sklearn: 1.5.2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, MaxAbsScaler, Normalizer\n",
    "from sklearn.preprocessing import PowerTransformer, QuantileTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "pd.set_option(\"display.max_columns\", 60)\n",
    "pd.set_option(\"display.width\", 160)\n",
    "\n",
    "print(\"Versions:\")\n",
    "import sklearn, sys\n",
    "print(\"  python:\", sys.version.split()[0])\n",
    "print(\"  sklearn:\", sklearn.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d8fb03",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Core concepts and notation\n",
    "\n",
    "Let a dataset have $n$ samples and $p$ features. We write the feature vector for sample $i$ as:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_i = [x_{i1}, x_{i2}, \\dots, x_{ip}]\n",
    "$$\n",
    "\n",
    "Two scaling operations you will use constantly:\n",
    "\n",
    "### Standardization (z-score scaling)\n",
    "\n",
    "For feature $j$:\n",
    "\n",
    "$$\n",
    "z_{ij} = \\frac{x_{ij} - \\mu_j}{\\sigma_j}\n",
    "$$\n",
    "\n",
    "- $\\mu_j$ is the mean of feature $j$ (computed on the **training** set).\n",
    "- $\\sigma_j$ is the standard deviation of feature $j$ (computed on the **training** set).\n",
    "\n",
    "### Min–max normalization\n",
    "\n",
    "For feature $j$:\n",
    "\n",
    "$$\n",
    "x'_{ij} = \\frac{x_{ij} - \\min_j}{\\max_j - \\min_j}\n",
    "$$\n",
    "\n",
    "This maps each feature (approximately) into $[0, 1]$.\n",
    "\n",
    "### Vector normalization (unit norm per sample)\n",
    "\n",
    "This normalizes the entire vector of a sample:\n",
    "\n",
    "$$\n",
    "\\tilde{\\mathbf{x}}_i = \\frac{\\mathbf{x}_i}{\\lVert \\mathbf{x}_i \\rVert_2}\n",
    "$$\n",
    "\n",
    "This is common in text retrieval / cosine similarity pipelines and is *different* from min–max.\n",
    "\n",
    "---\n",
    "\n",
    "## 2) When scaling matters (and when it does not)\n",
    "\n",
    "Scaling matters when your algorithm uses:\n",
    "\n",
    "- **Distances or inner products**: kNN, k-means, SVM (especially RBF kernel), PCA, kernel ridge regression.\n",
    "- **Regularization penalties** that assume comparable coefficient scales: Lasso, Ridge, Elastic Net.\n",
    "- **Coordinate-wise optimization** where step sizes depend on feature scale.\n",
    "\n",
    "Scaling is usually not essential for:\n",
    "\n",
    "- **Tree-based models**: decision trees, random forests, gradient boosting trees  \n",
    "  (splits are based on ordering; scale does not change order).\n",
    "- Some **rule-based** or **count-based** systems where features already share scale.\n",
    "\n",
    "However, “not essential” is not identical to “never useful.” Tree models can still benefit indirectly when scaling helps upstream steps (e.g., PCA features; stability of imputation; handling extreme ranges).\n",
    "\n",
    "---\n",
    "\n",
    "### A geometric intuition (distance dominance)\n",
    "\n",
    "Consider Euclidean distance:\n",
    "\n",
    "$$\n",
    "d(\\mathbf{x}, \\mathbf{y}) = \\sqrt{\\sum_{j=1}^{p} (x_j - y_j)^2}\n",
    "$$\n",
    "\n",
    "If one feature is measured in the range $[0, 10^6]$ and another in $[0, 1]$, then the large-range feature *dominates* the distance, regardless of whether it is informative. Scaling is a way to define what “distance” should mean for your task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffecb3d6",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Load multiple datasets (and look at feature scales)\n",
    "\n",
    "In real projects, feature scaling is rarely a “one dataset, one scaler” decision. You often build a reusable policy and then verify it across different data sources.\n",
    "\n",
    "We will use several datasets from your repository:\n",
    "\n",
    "- `diabetes.csv` (binary classification; numeric features with different ranges)\n",
    "- `iris.csv` (multiclass classification; classic demonstration for distance-based methods)\n",
    "- `Wine_Quality.csv` (tabular classification proxy; we will binarize quality for a clean example)\n",
    "- `drug200.csv` (mixed numeric + categorical; demonstrates column-wise pipelines)\n",
    "- `hw_200.csv` (clustering; demonstrates scaling + k-means / PCA)\n",
    "\n",
    "We will start by inspecting the raw scale of numeric columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bb54a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded shapes:\n",
      "  diabetes: (768, 9)\n",
      "  iris: (150, 5)\n",
      "  wine: (4898, 12)\n",
      "  drug: (200, 6)\n",
      "  hw_raw: (200, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>Diabetic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>Non-Diabetic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>Diabetic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>Non-Diabetic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>Diabetic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  DiabetesPedigreeFunction  Age classification\n",
       "0            6      148             72             35        0  33.6                     0.627   50       Diabetic\n",
       "1            1       85             66             29        0  26.6                     0.351   31   Non-Diabetic\n",
       "2            8      183             64              0        0  23.3                     0.672   32       Diabetic\n",
       "3            1       89             66             23       94  28.1                     0.167   21   Non-Diabetic\n",
       "4            0      137             40             35      168  43.1                     2.288   33       Diabetic"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Paths are relative to: Tutorials/English/Chapter2 or Tutorials/Persian/Chapter2\n",
    "p_diabetes = Path(\"../../../Datasets/Classification/diabetes.csv\")\n",
    "p_iris     = Path(\"../../../Datasets/Classification/iris.csv\")\n",
    "p_wine     = Path(\"../../../Datasets/Classification/Wine_Quality.csv\")\n",
    "p_drug     = Path(\"../../../Datasets/Classification/drug200.csv\")\n",
    "p_hw       = Path(\"../../../Datasets/Clustering/hw_200.csv\")\n",
    "\n",
    "diabetes = pd.read_csv(p_diabetes)\n",
    "iris = pd.read_csv(p_iris)\n",
    "wine = pd.read_csv(p_wine)\n",
    "drug = pd.read_csv(p_drug)\n",
    "hw_raw = pd.read_csv(p_hw)\n",
    "\n",
    "print(\"Loaded shapes:\")\n",
    "print(\"  diabetes:\", diabetes.shape)\n",
    "print(\"  iris:\", iris.shape)\n",
    "print(\"  wine:\", wine.shape)\n",
    "print(\"  drug:\", drug.shape)\n",
    "print(\"  hw_raw:\", hw_raw.shape)\n",
    "\n",
    "display(diabetes.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3853ba2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "diabetes: numeric scale summary (top by range)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>range</th>\n",
       "      <th>range_ratio_to_median</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Insulin</th>\n",
       "      <td>0.000</td>\n",
       "      <td>846.00</td>\n",
       "      <td>79.7995</td>\n",
       "      <td>115.1689</td>\n",
       "      <td>846.000</td>\n",
       "      <td>10.1866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Glucose</th>\n",
       "      <td>0.000</td>\n",
       "      <td>199.00</td>\n",
       "      <td>120.8945</td>\n",
       "      <td>31.9518</td>\n",
       "      <td>199.000</td>\n",
       "      <td>2.3961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BloodPressure</th>\n",
       "      <td>0.000</td>\n",
       "      <td>122.00</td>\n",
       "      <td>69.1055</td>\n",
       "      <td>19.3432</td>\n",
       "      <td>122.000</td>\n",
       "      <td>1.4690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SkinThickness</th>\n",
       "      <td>0.000</td>\n",
       "      <td>99.00</td>\n",
       "      <td>20.5365</td>\n",
       "      <td>15.9418</td>\n",
       "      <td>99.000</td>\n",
       "      <td>1.1921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BMI</th>\n",
       "      <td>0.000</td>\n",
       "      <td>67.10</td>\n",
       "      <td>31.9926</td>\n",
       "      <td>7.8790</td>\n",
       "      <td>67.100</td>\n",
       "      <td>0.8079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>21.000</td>\n",
       "      <td>81.00</td>\n",
       "      <td>33.2409</td>\n",
       "      <td>11.7526</td>\n",
       "      <td>60.000</td>\n",
       "      <td>0.7225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pregnancies</th>\n",
       "      <td>0.000</td>\n",
       "      <td>17.00</td>\n",
       "      <td>3.8451</td>\n",
       "      <td>3.3674</td>\n",
       "      <td>17.000</td>\n",
       "      <td>0.2047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <td>0.078</td>\n",
       "      <td>2.42</td>\n",
       "      <td>0.4719</td>\n",
       "      <td>0.3311</td>\n",
       "      <td>2.342</td>\n",
       "      <td>0.0282</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             min     max      mean       std    range  range_ratio_to_median\n",
       "Insulin                    0.000  846.00   79.7995  115.1689  846.000                10.1866\n",
       "Glucose                    0.000  199.00  120.8945   31.9518  199.000                 2.3961\n",
       "BloodPressure              0.000  122.00   69.1055   19.3432  122.000                 1.4690\n",
       "SkinThickness              0.000   99.00   20.5365   15.9418   99.000                 1.1921\n",
       "BMI                        0.000   67.10   31.9926    7.8790   67.100                 0.8079\n",
       "Age                       21.000   81.00   33.2409   11.7526   60.000                 0.7225\n",
       "Pregnancies                0.000   17.00    3.8451    3.3674   17.000                 0.2047\n",
       "DiabetesPedigreeFunction   0.078    2.42    0.4719    0.3311    2.342                 0.0282"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "iris: numeric scale summary (top by range)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>range</th>\n",
       "      <th>range_ratio_to_median</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>petal_length</th>\n",
       "      <td>1.0</td>\n",
       "      <td>6.9</td>\n",
       "      <td>3.7587</td>\n",
       "      <td>1.7585</td>\n",
       "      <td>5.9</td>\n",
       "      <td>1.9667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sepal_length</th>\n",
       "      <td>4.3</td>\n",
       "      <td>7.9</td>\n",
       "      <td>5.8433</td>\n",
       "      <td>0.8253</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sepal_width</th>\n",
       "      <td>2.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>3.0540</td>\n",
       "      <td>0.4321</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.8000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>petal_width</th>\n",
       "      <td>0.1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1.1987</td>\n",
       "      <td>0.7606</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.8000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              min  max    mean     std  range  range_ratio_to_median\n",
       "petal_length  1.0  6.9  3.7587  1.7585    5.9                 1.9667\n",
       "sepal_length  4.3  7.9  5.8433  0.8253    3.6                 1.2000\n",
       "sepal_width   2.0  4.4  3.0540  0.4321    2.4                 0.8000\n",
       "petal_width   0.1  2.5  1.1987  0.7606    2.4                 0.8000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "wine: numeric scale summary (top by range)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>range</th>\n",
       "      <th>range_ratio_to_median</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <td>9.00</td>\n",
       "      <td>440.00</td>\n",
       "      <td>138.3607</td>\n",
       "      <td>42.4937</td>\n",
       "      <td>431.00</td>\n",
       "      <td>112.5326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <td>2.00</td>\n",
       "      <td>289.00</td>\n",
       "      <td>35.3081</td>\n",
       "      <td>17.0054</td>\n",
       "      <td>287.00</td>\n",
       "      <td>74.9347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>residual sugar</th>\n",
       "      <td>0.60</td>\n",
       "      <td>65.80</td>\n",
       "      <td>6.3914</td>\n",
       "      <td>5.0715</td>\n",
       "      <td>65.20</td>\n",
       "      <td>17.0235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fixed acidity</th>\n",
       "      <td>3.80</td>\n",
       "      <td>14.20</td>\n",
       "      <td>6.8548</td>\n",
       "      <td>0.8438</td>\n",
       "      <td>10.40</td>\n",
       "      <td>2.7154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alcohol</th>\n",
       "      <td>8.00</td>\n",
       "      <td>14.20</td>\n",
       "      <td>10.5143</td>\n",
       "      <td>1.2305</td>\n",
       "      <td>6.20</td>\n",
       "      <td>1.6188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality</th>\n",
       "      <td>3.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>5.8779</td>\n",
       "      <td>0.8855</td>\n",
       "      <td>6.00</td>\n",
       "      <td>1.5666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>citric acid</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.66</td>\n",
       "      <td>0.3342</td>\n",
       "      <td>0.1210</td>\n",
       "      <td>1.66</td>\n",
       "      <td>0.4334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pH</th>\n",
       "      <td>2.72</td>\n",
       "      <td>3.82</td>\n",
       "      <td>3.1883</td>\n",
       "      <td>0.1510</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.2872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>volatile acidity</th>\n",
       "      <td>0.08</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.2782</td>\n",
       "      <td>0.1008</td>\n",
       "      <td>1.02</td>\n",
       "      <td>0.2663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sulphates</th>\n",
       "      <td>0.22</td>\n",
       "      <td>1.08</td>\n",
       "      <td>0.4898</td>\n",
       "      <td>0.1141</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.2245</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       min     max      mean      std   range  range_ratio_to_median\n",
       "total sulfur dioxide  9.00  440.00  138.3607  42.4937  431.00               112.5326\n",
       "free sulfur dioxide   2.00  289.00   35.3081  17.0054  287.00                74.9347\n",
       "residual sugar        0.60   65.80    6.3914   5.0715   65.20                17.0235\n",
       "fixed acidity         3.80   14.20    6.8548   0.8438   10.40                 2.7154\n",
       "alcohol               8.00   14.20   10.5143   1.2305    6.20                 1.6188\n",
       "quality               3.00    9.00    5.8779   0.8855    6.00                 1.5666\n",
       "citric acid           0.00    1.66    0.3342   0.1210    1.66                 0.4334\n",
       "pH                    2.72    3.82    3.1883   0.1510    1.10                 0.2872\n",
       "volatile acidity      0.08    1.10    0.2782   0.1008    1.02                 0.2663\n",
       "sulphates             0.22    1.08    0.4898   0.1141    0.86                 0.2245"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def scale_summary(df: pd.DataFrame, name: str, max_cols: int = 10):\n",
    "    num = df.select_dtypes(include=[np.number])\n",
    "    if num.empty:\n",
    "        print(f\"{name}: no numeric columns\")\n",
    "        return\n",
    "    s = pd.DataFrame({\n",
    "        \"min\": num.min(),\n",
    "        \"max\": num.max(),\n",
    "        \"mean\": num.mean(),\n",
    "        \"std\": num.std(ddof=0),\n",
    "    })\n",
    "    s[\"range\"] = s[\"max\"] - s[\"min\"]\n",
    "    s[\"range_ratio_to_median\"] = s[\"range\"] / (np.median(s[\"range\"]) + 1e-12)\n",
    "    s = s.sort_values(\"range\", ascending=False)\n",
    "    print(f\"\\n{name}: numeric scale summary (top by range)\")\n",
    "    display(s.head(max_cols).round(4))\n",
    "\n",
    "scale_summary(diabetes, \"diabetes\")\n",
    "scale_summary(iris, \"iris\")\n",
    "scale_summary(wine, \"wine\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe3cbbb",
   "metadata": {},
   "source": [
    "\n",
    "### Interpretation\n",
    "\n",
    "Even if each dataset looks “numeric”, the feature scales differ:\n",
    "\n",
    "- In `diabetes`, **Insulin** and **Glucose** are on a completely different range than **DiabetesPedigreeFunction**.\n",
    "- In `wine`, some chemistry measures have very different spreads (e.g., sulphates vs free sulfur dioxide).\n",
    "- In `iris`, the ranges are moderate, but scaling can still change the neighborhood geometry.\n",
    "\n",
    "This is where scaling decisions begin.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74acfda4",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Standardization in practice (z-score) — and why it helps optimization\n",
    "\n",
    "Many algorithms effectively solve an optimization problem. For example, logistic regression (with L2 regularization) can be written as:\n",
    "\n",
    "$$\n",
    "\\min_{\\mathbf{w}, b} \\; \\frac{1}{n}\\sum_{i=1}^n \\log\\left(1 + \\exp\\left(-y_i(\\mathbf{w}^\\top \\mathbf{x}_i + b)\\right)\\right) + \\lambda \\lVert \\mathbf{w} \\rVert_2^2\n",
    "$$\n",
    "\n",
    "If one feature is 1000× larger than another, then the loss landscape becomes **ill-conditioned**, and the optimizer may need tiny steps in one direction and large steps in another. Standardization makes the problem closer to “spherical” and improves convergence behavior.\n",
    "\n",
    "### Example: logistic regression on the diabetes dataset\n",
    "\n",
    "We will compare:\n",
    "\n",
    "- Model A: Logistic Regression **without** scaling\n",
    "- Model B: Logistic Regression **with** `StandardScaler` inside a `Pipeline`\n",
    "\n",
    "We will evaluate on a held-out test split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c472b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['Diabetic', 'Non-Diabetic']\n",
      "\n",
      "Accuracy (no scaling): 0.7812\n",
      "Accuracy (standardized): 0.7865\n",
      "\n",
      "Confusion matrix (no scaling):\n",
      " [[ 39  28]\n",
      " [ 14 111]]\n",
      "\n",
      "Confusion matrix (standardized):\n",
      " [[ 40  27]\n",
      " [ 14 111]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy (no scaling): 0.4933\n",
      "Accuracy (standardized): 0.5\n",
      "\n",
      "Confusion matrix (no scaling):\n",
      " [[59 23]\n",
      " [53 15]]\n",
      "\n",
      "Confusion matrix (standardized):\n",
      " [[63 19]\n",
      " [56 12]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "X = diabetes.drop(columns=[\"classification\"])\n",
    "y = diabetes[\"classification\"].copy()\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_bin = le.fit_transform(y)\n",
    "print(\"Classes:\", list(le.classes_))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_bin, test_size=0.25, random_state=42, stratify=y_bin\n",
    ")\n",
    "\n",
    "# A) No scaling\n",
    "lr_raw = LogisticRegression(max_iter=2000, solver=\"lbfgs\")\n",
    "lr_raw.fit(X_train, y_train)\n",
    "pred_raw = lr_raw.predict(X_test)\n",
    "\n",
    "# B) With standardization\n",
    "lr_scaled = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"clf\", LogisticRegression(max_iter=2000, solver=\"lbfgs\"))\n",
    "])\n",
    "lr_scaled.fit(X_train, y_train)\n",
    "pred_scaled = lr_scaled.predict(X_test)\n",
    "\n",
    "print(\"\\nAccuracy (no scaling):\", round(accuracy_score(y_test, pred_raw), 4))\n",
    "print(\"Accuracy (standardized):\", round(accuracy_score(y_test, pred_scaled), 4))\n",
    "\n",
    "print(\"\\nConfusion matrix (no scaling):\\n\", confusion_matrix(y_test, pred_raw))\n",
    "print(\"\\nConfusion matrix (standardized):\\n\", confusion_matrix(y_test, pred_scaled))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296ff4df",
   "metadata": {},
   "source": [
    "\n",
    "### Coefficients and interpretability: scaling changes the meaning\n",
    "\n",
    "If you fit a linear model *without* scaling, the coefficient magnitude is influenced by the unit of measurement.\n",
    "\n",
    "Standardization makes coefficients more directly comparable as “effect per standard deviation,” which is often closer to what practitioners mean when they talk about *relative importance* in linear models.\n",
    "\n",
    "Let’s inspect coefficients from the standardized model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9a373b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef (standardized)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Glucose</th>\n",
       "      <td>-1.1240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BMI</th>\n",
       "      <td>-0.6716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pregnancies</th>\n",
       "      <td>-0.4667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BloodPressure</th>\n",
       "      <td>0.3129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <td>-0.2806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Insulin</th>\n",
       "      <td>0.1773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>-0.1613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SkinThickness</th>\n",
       "      <td>-0.1089</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          coef (standardized)\n",
       "Glucose                               -1.1240\n",
       "BMI                                   -0.6716\n",
       "Pregnancies                           -0.4667\n",
       "BloodPressure                          0.3129\n",
       "DiabetesPedigreeFunction              -0.2806\n",
       "Insulin                                0.1773\n",
       "Age                                   -0.1613\n",
       "SkinThickness                         -0.1089"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpretation: coefficients after StandardScaler are roughly 'effect per 1 std' of the feature.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "feature_names = X.columns.tolist()\n",
    "\n",
    "coef = lr_scaled.named_steps[\"clf\"].coef_.ravel()\n",
    "coef_s = pd.Series(coef, index=feature_names).sort_values(key=lambda s: np.abs(s), ascending=False)\n",
    "\n",
    "display(coef_s.to_frame(\"coef (standardized)\").head(10).round(4))\n",
    "print(\"Interpretation: coefficients after StandardScaler are roughly 'effect per 1 std' of the feature.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d1e764",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Min–max normalization vs standardization — practical tradeoffs\n",
    "\n",
    "### When min–max is useful\n",
    "\n",
    "Min–max scaling is common when:\n",
    "\n",
    "- You want to keep features in a bounded range $[0, 1]$.\n",
    "- Your model has constraints or priors that expect bounded inputs.\n",
    "- You want to preserve **relative** spacing while controlling range (important in some distance-based methods).\n",
    "\n",
    "### When standardization is more robust\n",
    "\n",
    "Standardization often works better when:\n",
    "\n",
    "- Features are roughly bell-shaped or you want to treat them as such.\n",
    "- You use regularized linear models, SVM, or PCA.\n",
    "- You want centering around zero (important for many optimizers).\n",
    "\n",
    "### A critical warning (outliers)\n",
    "\n",
    "Min–max scaling is sensitive to outliers: one extreme value can compress the entire feature into a tiny interval. Robust scalers are often better for heavy-tailed data.\n",
    "\n",
    "---\n",
    "\n",
    "## 6) Demonstration: kNN on the Iris dataset (no scaling vs min–max vs standardization)\n",
    "\n",
    "kNN is a distance-based method. If a feature has a larger range, it gets a larger weight implicitly.\n",
    "\n",
    "We will compare:\n",
    "\n",
    "- kNN on raw features\n",
    "- kNN with `MinMaxScaler`\n",
    "- kNN with `StandardScaler`\n",
    "\n",
    "To avoid leakage, scaling is inside `Pipeline`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40af1fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     raw  accuracy = 0.9474\n",
      "  minmax  accuracy = 0.9737\n",
      "standard  accuracy = 0.9474\n",
      "\n",
      "A short classification report for the standardized pipeline:\n",
      "\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    Iris-setosa       1.00      1.00      1.00        12\n",
      "Iris-versicolor       0.87      1.00      0.93        13\n",
      " Iris-virginica       1.00      0.85      0.92        13\n",
      "\n",
      "       accuracy                           0.95        38\n",
      "      macro avg       0.96      0.95      0.95        38\n",
      "   weighted avg       0.95      0.95      0.95        38\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  minmax  accuracy = 0.9737\n",
      "standard  accuracy = 0.9474\n",
      "\n",
      "A short classification report for the standardized pipeline:\n",
      "\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    Iris-setosa       1.00      1.00      1.00        12\n",
      "Iris-versicolor       0.87      1.00      0.93        13\n",
      " Iris-virginica       1.00      0.85      0.92        13\n",
      "\n",
      "       accuracy                           0.95        38\n",
      "      macro avg       0.96      0.95      0.95        38\n",
      "   weighted avg       0.95      0.95      0.95        38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = iris.drop(columns=[\"classification\"])\n",
    "y = iris[\"classification\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "pipelines = {\n",
    "    \"raw\": Pipeline([(\"knn\", KNeighborsClassifier(n_neighbors=7))]),\n",
    "    \"minmax\": Pipeline([(\"scaler\", MinMaxScaler()), (\"knn\", KNeighborsClassifier(n_neighbors=7))]),\n",
    "    \"standard\": Pipeline([(\"scaler\", StandardScaler()), (\"knn\", KNeighborsClassifier(n_neighbors=7))]),\n",
    "}\n",
    "\n",
    "for name, pipe in pipelines.items():\n",
    "    pipe.fit(X_train, y_train)\n",
    "    pred = pipe.predict(X_test)\n",
    "    acc = accuracy_score(y_test, pred)\n",
    "    print(f\"{name:>8}  accuracy = {acc:.4f}\")\n",
    "\n",
    "print(\"\\nA short classification report for the standardized pipeline:\\n\")\n",
    "pred_std = pipelines[\"standard\"].predict(X_test)\n",
    "print(classification_report(y_test, pred_std))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1676c42",
   "metadata": {},
   "source": [
    "\n",
    "### Discussion (kNN)\n",
    "\n",
    "In practice, kNN performance can change materially after scaling, but note:\n",
    "\n",
    "- The “best scaler” can depend on $k$, distance metric, and the dataset.\n",
    "- Scaling is not about making results *always better*; it is about making the method behave as intended.\n",
    "- The correct workflow is to treat the scaler as a hyperparameter and validate it.\n",
    "\n",
    "Let’s demonstrate this principle using cross-validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff2be302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     raw  mean=0.9600  std=0.0389  scores=[1.     0.9667 0.9    1.     0.9333]\n",
      "  minmax  mean=0.9533  std=0.0452  scores=[1.     0.9667 0.9    1.     0.9   ]\n",
      "standard  mean=0.9600  std=0.0327  scores=[0.9667 0.9667 0.9    1.     0.9667]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  minmax  mean=0.9533  std=0.0452  scores=[1.     0.9667 0.9    1.     0.9   ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standard  mean=0.9600  std=0.0327  scores=[0.9667 0.9667 0.9    1.     0.9667]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for name, pipe in pipelines.items():\n",
    "    scores = cross_val_score(pipe, X, y, cv=cv, scoring=\"accuracy\")\n",
    "    print(f\"{name:>8}  mean={scores.mean():.4f}  std={scores.std():.4f}  scores={np.round(scores,4)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4555273",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Outliers and heavy tails: RobustScaler, PowerTransformer, QuantileTransformer\n",
    "\n",
    "Real data is often not “nice.” Outliers can make standard deviation unstable and can break min–max scaling. You have several robust options:\n",
    "\n",
    "### RobustScaler\n",
    "Uses median and interquartile range (IQR):\n",
    "\n",
    "$$\n",
    "x' = \\frac{x - \\text{median}(x)}{\\text{IQR}(x)}\n",
    "$$\n",
    "\n",
    "### PowerTransformer (Yeo–Johnson / Box–Cox)\n",
    "Transforms data to be more Gaussian-like, often improving linear model behavior.\n",
    "\n",
    "### QuantileTransformer\n",
    "Maps data to a target distribution (uniform or normal) using quantiles. It can be effective but can also distort distances; validate carefully.\n",
    "\n",
    "We will demonstrate with the wine dataset, where some features can be skewed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8dfbc74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             raw  accuracy=0.8090  positive_rate_train=0.216\n",
      "        standard  accuracy=0.8073  positive_rate_train=0.216\n",
      "          robust  accuracy=0.8024  positive_rate_train=0.216\n",
      "power_yeojohnson  accuracy=0.8049  positive_rate_train=0.216\n",
      " quantile_normal  accuracy=0.8049  positive_rate_train=0.216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        standard  accuracy=0.7367  positive_rate_train=0.266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          robust  accuracy=0.7367  positive_rate_train=0.266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "power_yeojohnson  accuracy=0.7367  positive_rate_train=0.266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " quantile_normal  accuracy=0.7367  positive_rate_train=0.266\n"
     ]
    }
   ],
   "source": [
    "\n",
    "wine_y = (wine[\"quality\"] >= 7).astype(int)\n",
    "wine_X = wine.drop(columns=[\"quality\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    wine_X, wine_y, test_size=0.25, random_state=42, stratify=wine_y\n",
    ")\n",
    "\n",
    "scaler_pipes = {\n",
    "    \"raw\": Pipeline([(\"clf\", LogisticRegression(max_iter=4000))]),\n",
    "    \"standard\": Pipeline([(\"scaler\", StandardScaler()), (\"clf\", LogisticRegression(max_iter=4000))]),\n",
    "    \"robust\": Pipeline([(\"scaler\", RobustScaler()), (\"clf\", LogisticRegression(max_iter=4000))]),\n",
    "    \"power_yeojohnson\": Pipeline([(\"scaler\", PowerTransformer(method=\"yeo-johnson\", standardize=True)), (\"clf\", LogisticRegression(max_iter=4000))]),\n",
    "    \"quantile_normal\": Pipeline([(\"scaler\", QuantileTransformer(output_distribution=\"normal\", n_quantiles=200, random_state=42)), (\"clf\", LogisticRegression(max_iter=4000))]),\n",
    "}\n",
    "\n",
    "for name, pipe in scaler_pipes.items():\n",
    "    pipe.fit(X_train, y_train)\n",
    "    pred = pipe.predict(X_test)\n",
    "    acc = accuracy_score(y_test, pred)\n",
    "    print(f\"{name:>16}  accuracy={acc:.4f}  positive_rate_train={y_train.mean():.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14ee192",
   "metadata": {},
   "source": [
    "\n",
    "### Takeaway (robust / power / quantile)\n",
    "\n",
    "- RobustScaler is a strong default when outliers exist but you still want a linear-ish interpretation.\n",
    "- Power transforms can be beneficial when skewness is strong and you want a model that behaves “more linearly.”\n",
    "- Quantile transforms can work surprisingly well, but because they change the metric structure, you should validate them with CV and watch for overfitting on small datasets.\n",
    "\n",
    "---\n",
    "\n",
    "## 8) Scaling + SVM: why “C” and “gamma” depend on feature scale\n",
    "\n",
    "For an RBF SVM, the kernel is:\n",
    "\n",
    "$$\n",
    "K(\\mathbf{x}, \\mathbf{y}) = \\exp(-\\gamma \\lVert \\mathbf{x} - \\mathbf{y} \\rVert_2^2)\n",
    "$$\n",
    "\n",
    "If you scale features, the distances change, and therefore the “effective” meaning of $\\gamma$ changes.\n",
    "\n",
    "This is why **SVM is almost always used with scaling**, and why hyperparameter search should be done with scaling included in the pipeline.\n",
    "\n",
    "We will demonstrate this on `iris` with a modest SVM configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b95b730c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM (no scaling) accuracy: 0.9737\n",
      "SVM (standardized) accuracy: 0.9474\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = iris.drop(columns=[\"classification\"])\n",
    "y = iris[\"classification\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "svm_raw = SVC(kernel=\"rbf\", C=3.0, gamma=\"scale\")\n",
    "svm_scaled = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svm\", SVC(kernel=\"rbf\", C=3.0, gamma=\"scale\"))\n",
    "])\n",
    "\n",
    "svm_raw.fit(X_train, y_train)\n",
    "svm_scaled.fit(X_train, y_train)\n",
    "\n",
    "pred_raw = svm_raw.predict(X_test)\n",
    "pred_scaled = svm_scaled.predict(X_test)\n",
    "\n",
    "print(\"SVM (no scaling) accuracy:\", round(accuracy_score(y_test, pred_raw), 4))\n",
    "print(\"SVM (standardized) accuracy:\", round(accuracy_score(y_test, pred_scaled), 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cf41a7",
   "metadata": {},
   "source": [
    "\n",
    "## 9) Data leakage: the most common scaling mistake\n",
    "\n",
    "**Leakage** happens when information from the test set influences the training process.\n",
    "\n",
    "A subtle scaling leakage looks like this:\n",
    "\n",
    "1. Fit `StandardScaler` on *all data*\n",
    "2. Transform train and test using that scaler\n",
    "3. Evaluate\n",
    "\n",
    "This is wrong because the scaler’s mean and standard deviation used information from the test set.\n",
    "\n",
    "We will demonstrate the difference between:\n",
    "\n",
    "- Incorrect scaling (fit scaler on full data)\n",
    "- Correct scaling (fit scaler on train only) using `Pipeline`\n",
    "\n",
    "We’ll use the diabetes dataset again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdac57ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with leakage (wrong): 0.7865\n",
      "Accuracy without leakage (right): 0.7865\n",
      "\n",
      "Correct scaler mean (first 3 features): [  3.856 121.705  69.559]\n",
      "Correct scaler var  (first 3 features): [  11.991 1056.913  356.729]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = diabetes.drop(columns=[\"classification\"])\n",
    "y = le.fit_transform(diabetes[\"classification\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# WRONG: fit scaler on all data, then split transformed data\n",
    "scaler_wrong = StandardScaler()\n",
    "X_all_scaled = scaler_wrong.fit_transform(X)\n",
    "\n",
    "Xa_train, Xa_test, ya_train, ya_test = train_test_split(\n",
    "    X_all_scaled, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "lr_wrong = LogisticRegression(max_iter=2000)\n",
    "lr_wrong.fit(Xa_train, ya_train)\n",
    "pred_wrong = lr_wrong.predict(Xa_test)\n",
    "acc_wrong = accuracy_score(ya_test, pred_wrong)\n",
    "\n",
    "# RIGHT: scaler inside pipeline fitted on training only\n",
    "lr_right = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"clf\", LogisticRegression(max_iter=2000))\n",
    "])\n",
    "lr_right.fit(X_train, y_train)\n",
    "pred_right = lr_right.predict(X_test)\n",
    "acc_right = accuracy_score(y_test, pred_right)\n",
    "\n",
    "print(\"Accuracy with leakage (wrong):\", round(acc_wrong, 4))\n",
    "print(\"Accuracy without leakage (right):\", round(acc_right, 4))\n",
    "\n",
    "# Additional sanity check: show that correct scaler stats come ONLY from training\n",
    "sc = lr_right.named_steps[\"scaler\"]\n",
    "print(\"\\nCorrect scaler mean (first 3 features):\", np.round(sc.mean_[:3], 3))\n",
    "print(\"Correct scaler var  (first 3 features):\", np.round(sc.var_[:3], 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5392448",
   "metadata": {},
   "source": [
    "\n",
    "### Professional rule (non-negotiable)\n",
    "\n",
    "If scaling exists at all, it must be fitted **inside** the training process:\n",
    "\n",
    "- Use `Pipeline` and validate with cross-validation.\n",
    "- In production, persist the fitted scaler together with the model (one artifact).\n",
    "\n",
    "---\n",
    "\n",
    "## 10) Mixed data types: scale numeric, encode categoricals (drug200 example)\n",
    "\n",
    "In tabular ML, it is common to have:\n",
    "\n",
    "- Numeric features that require scaling (e.g., `Age`, `Na_to_K`)\n",
    "- Categorical features that require encoding (`Sex`, `BP`, `Cholesterol`)\n",
    "\n",
    "The correct approach is to build a **column-wise** pipeline:\n",
    "\n",
    "- Numeric: `SimpleImputer` → `StandardScaler`\n",
    "- Categorical: `SimpleImputer` → `OneHotEncoder`\n",
    "- Model: a classifier (e.g., logistic regression)\n",
    "\n",
    "We will do multiclass prediction for `Drug`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed32ca59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.92\n",
      "\n",
      "Class distribution (test):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>share</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Drug</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DrugY</th>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>drugX</th>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>drugA</th>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>drugC</th>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>drugB</th>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       share\n",
       "Drug        \n",
       "DrugY   0.46\n",
       "drugX   0.26\n",
       "drugA   0.12\n",
       "drugC   0.08\n",
       "drugB   0.08"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       DrugY       0.88      0.96      0.92        23\n",
      "       drugA       1.00      1.00      1.00         6\n",
      "       drugB       1.00      0.50      0.67         4\n",
      "       drugC       1.00      1.00      1.00         4\n",
      "       drugX       0.92      0.92      0.92        13\n",
      "\n",
      "    accuracy                           0.92        50\n",
      "   macro avg       0.96      0.88      0.90        50\n",
      "weighted avg       0.92      0.92      0.92        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = drug.drop(columns=[\"Drug\"])\n",
    "y = drug[\"Drug\"]\n",
    "\n",
    "numeric_features = [\"Age\", \"Na_to_K\"]\n",
    "categorical_features = [\"Sex\", \"BP\", \"Cholesterol\"]\n",
    "\n",
    "numeric_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_pipe, numeric_features),\n",
    "        (\"cat\", categorical_pipe, categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "clf = LogisticRegression(max_iter=4000)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"prep\", preprocess),\n",
    "    (\"clf\", clf)\n",
    "])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "pred = pipe.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", round(accuracy_score(y_test, pred), 4))\n",
    "print(\"\\nClass distribution (test):\")\n",
    "display(pd.Series(y_test).value_counts(normalize=True).to_frame(\"share\").round(3))\n",
    "\n",
    "print(\"\\nClassification report:\\n\")\n",
    "print(classification_report(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c9e0a4",
   "metadata": {},
   "source": [
    "\n",
    "## 11) Sparse matrices and scaling: StandardScaler vs MaxAbsScaler\n",
    "\n",
    "One-hot encoding produces a sparse design matrix. Two important notes:\n",
    "\n",
    "- `StandardScaler(with_mean=True)` cannot be applied directly to sparse matrices (centering would densify the matrix).\n",
    "- If you need scaling with sparse features, prefer:\n",
    "  - `StandardScaler(with_mean=False)` (keeps sparsity), or\n",
    "  - `MaxAbsScaler` (scales by max absolute value; preserves sparsity).\n",
    "\n",
    "We will construct a quick “mostly sparse” design via one-hot encoding and compare scalers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "264208e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed shape: (120, 7)\n",
      "Sparse output: False\n",
      "Sparsity (nnz / total): 360 / 840\n",
      "\n",
      "StandardScaler(with_mean=True) succeeded (likely dense input).\n",
      "\n",
      "After StandardScaler(with_mean=False):\n",
      "  is_sparse: False\n",
      "\n",
      "After MaxAbsScaler:\n",
      "  is_sparse: False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from scipy import sparse\n",
    "\n",
    "toy = drug.sample(120, random_state=0).reset_index(drop=True)\n",
    "X = toy.drop(columns=[\"Drug\"])\n",
    "\n",
    "# Force sparse output for compatibility across scikit-learn versions\n",
    "try:\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True)\n",
    "except TypeError:\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=True)\n",
    "\n",
    "pre_cat_only = ColumnTransformer(\n",
    "    transformers=[(\"cat\", ohe, [\"Sex\",\"BP\",\"Cholesterol\"])],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "X_sparse = pre_cat_only.fit_transform(X)\n",
    "\n",
    "is_sp = sparse.issparse(X_sparse)\n",
    "nnz = X_sparse.nnz if is_sp else int(np.count_nonzero(X_sparse))\n",
    "total = int(X_sparse.shape[0] * X_sparse.shape[1])\n",
    "\n",
    "print(\"Transformed shape:\", X_sparse.shape)\n",
    "print(\"Sparse output:\", is_sp)\n",
    "print(\"Sparsity (nnz / total):\", nnz, \"/\", total)\n",
    "\n",
    "# Demonstrate why centering is problematic for sparse matrices\n",
    "try:\n",
    "    _ = StandardScaler(with_mean=True).fit_transform(X_sparse)\n",
    "    print(\"\\nStandardScaler(with_mean=True) succeeded (likely dense input).\")\n",
    "except Exception as e:\n",
    "    print(\"\\nStandardScaler(with_mean=True) on sparse -> error type:\", type(e).__name__)\n",
    "    print(\"Message (short):\", str(e).splitlines()[0][:140])\n",
    "\n",
    "X_std_no_mean = StandardScaler(with_mean=False).fit_transform(X_sparse)\n",
    "X_maxabs = MaxAbsScaler().fit_transform(X_sparse)\n",
    "\n",
    "print(\"\\nAfter StandardScaler(with_mean=False):\")\n",
    "print(\"  is_sparse:\", sparse.issparse(X_std_no_mean))\n",
    "\n",
    "print(\"\\nAfter MaxAbsScaler:\")\n",
    "print(\"  is_sparse:\", sparse.issparse(X_maxabs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7330675a",
   "metadata": {},
   "source": [
    "\n",
    "## 12) Scaling for PCA and k-means (clustering example with hw_200)\n",
    "\n",
    "Both PCA and k-means rely on Euclidean geometry:\n",
    "\n",
    "- PCA finds directions of maximum variance. If one feature has larger units, it dominates principal components.\n",
    "- k-means minimizes within-cluster sum of squares, which depends directly on feature scale.\n",
    "\n",
    "We will use the `hw_200.csv` dataset (height/weight). It has intentionally messy column names.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Load the dataset\n",
    "2. Clean column names\n",
    "3. Compare PCA and k-means behavior with and without scaling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5408c879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original columns: ['Index', ' Height(Inches)\"', ' \"Weight(Pounds)\"']\n",
      "Cleaned columns: ['Index', 'Height(Inches)', 'Weight(Pounds)']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>Height(Inches)</th>\n",
       "      <th>Weight(Pounds)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>65.78</td>\n",
       "      <td>112.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>71.52</td>\n",
       "      <td>136.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>69.40</td>\n",
       "      <td>153.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>68.22</td>\n",
       "      <td>142.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>67.79</td>\n",
       "      <td>144.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Index  Height(Inches)  Weight(Pounds)\n",
       "0      1           65.78          112.99\n",
       "1      2           71.52          136.49\n",
       "2      3           69.40          153.03\n",
       "3      4           68.22          142.34\n",
       "4      5           67.79          144.30"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X_hw shape: (200, 2)\n",
      "Feature means (raw): [ 67.9498 127.222 ]\n",
      "Feature stds  (raw): [ 1.9355 11.931 ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "hw = hw_raw.copy()\n",
    "print(\"Original columns:\", list(hw.columns))\n",
    "\n",
    "hw.columns = [c.replace('\"', '').strip() for c in hw.columns]\n",
    "hw.columns = [re.sub(r\"\\s+\", \" \", c).strip() for c in hw.columns]\n",
    "print(\"Cleaned columns:\", list(hw.columns))\n",
    "\n",
    "display(hw.head())\n",
    "\n",
    "# robustly pick height/weight columns\n",
    "hw_cols = [c for c in hw.columns if \"Height\" in c or \"Weight\" in c]\n",
    "X_hw = hw[hw_cols].astype(float).values\n",
    "\n",
    "print(\"\\nX_hw shape:\", X_hw.shape)\n",
    "print(\"Feature means (raw):\", np.round(X_hw.mean(axis=0), 4))\n",
    "print(\"Feature stds  (raw):\", np.round(X_hw.std(axis=0), 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04814129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance ratio (PCA raw): [0.9825 0.0175]\n",
      "Explained variance ratio (PCA scaled): [0.7784 0.2216]\n",
      "\n",
      "Cluster sizes (raw): [93 44 63]\n",
      "Cluster sizes (scaled): [51 59 90]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>raw_c0</th>\n",
       "      <td>67.59</td>\n",
       "      <td>126.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>raw_c1</th>\n",
       "      <td>66.66</td>\n",
       "      <td>110.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>raw_c2</th>\n",
       "      <td>69.37</td>\n",
       "      <td>140.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scaled_c0_back</th>\n",
       "      <td>70.30</td>\n",
       "      <td>139.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scaled_c1_back</th>\n",
       "      <td>65.99</td>\n",
       "      <td>114.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scaled_c2_back</th>\n",
       "      <td>67.90</td>\n",
       "      <td>128.78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Height  Weight\n",
       "raw_c0           67.59  126.05\n",
       "raw_c1           66.66  110.46\n",
       "raw_c2           69.37  140.66\n",
       "scaled_c0_back   70.30  139.28\n",
       "scaled_c1_back   65.99  114.42\n",
       "scaled_c2_back   67.90  128.78"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>raw_c0</th>\n",
       "      <td>69.50</td>\n",
       "      <td>165.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>raw_c1</th>\n",
       "      <td>66.54</td>\n",
       "      <td>125.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>raw_c2</th>\n",
       "      <td>68.53</td>\n",
       "      <td>144.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scaled_c0_back</th>\n",
       "      <td>71.38</td>\n",
       "      <td>159.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scaled_c1_back</th>\n",
       "      <td>66.46</td>\n",
       "      <td>130.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scaled_c2_back</th>\n",
       "      <td>66.79</td>\n",
       "      <td>155.81</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Height  Weight\n",
       "raw_c0           69.50  165.66\n",
       "raw_c1           66.54  125.63\n",
       "raw_c2           68.53  144.99\n",
       "scaled_c0_back   71.38  159.98\n",
       "scaled_c1_back   66.46  130.81\n",
       "scaled_c2_back   66.79  155.81"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# PCA without scaling\n",
    "pca_raw = PCA(n_components=2, random_state=0)\n",
    "Z_raw = pca_raw.fit_transform(X_hw)\n",
    "\n",
    "# PCA with scaling\n",
    "X_hw_scaled = scaler.fit_transform(X_hw)\n",
    "pca_scaled = PCA(n_components=2, random_state=0)\n",
    "Z_scaled = pca_scaled.fit_transform(X_hw_scaled)\n",
    "\n",
    "print(\"Explained variance ratio (PCA raw):\", np.round(pca_raw.explained_variance_ratio_, 4))\n",
    "print(\"Explained variance ratio (PCA scaled):\", np.round(pca_scaled.explained_variance_ratio_, 4))\n",
    "\n",
    "km_raw = KMeans(n_clusters=3, random_state=0, n_init=10)\n",
    "labels_raw = km_raw.fit_predict(X_hw)\n",
    "\n",
    "km_scaled = KMeans(n_clusters=3, random_state=0, n_init=10)\n",
    "labels_scaled = km_scaled.fit_predict(X_hw_scaled)\n",
    "\n",
    "print(\"\\nCluster sizes (raw):\", np.bincount(labels_raw))\n",
    "print(\"Cluster sizes (scaled):\", np.bincount(labels_scaled))\n",
    "\n",
    "centers_raw = km_raw.cluster_centers_\n",
    "centers_scaled_back = scaler.inverse_transform(km_scaled.cluster_centers_)\n",
    "\n",
    "centers_df = pd.DataFrame(\n",
    "    np.vstack([centers_raw, centers_scaled_back]),\n",
    "    columns=[c.replace(\"(Inches)\", \"\").replace(\"(Pounds)\", \"\") for c in hw_cols]\n",
    ")\n",
    "centers_df.index = [\"raw_c0\",\"raw_c1\",\"raw_c2\",\"scaled_c0_back\",\"scaled_c1_back\",\"scaled_c2_back\"]\n",
    "display(centers_df.round(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202f3a44",
   "metadata": {},
   "source": [
    "\n",
    "## 13) Feature scaling and regularization: why Ridge depends on units\n",
    "\n",
    "Ridge regression:\n",
    "\n",
    "$$\n",
    "\\min_{\\mathbf{w}} \\; \\frac{1}{n}\\sum_{i=1}^n (y_i - \\mathbf{w}^\\top \\mathbf{x}_i)^2 + \\lambda \\lVert \\mathbf{w} \\rVert_2^2\n",
    "$$\n",
    "\n",
    "Scaling makes the regularization term behave more “fairly” across features.\n",
    "\n",
    "### Example: Ridge regression on house prices\n",
    "\n",
    "We will build a mixed-type pipeline (numeric scaling + categorical one-hot) and compare Ridge with and without numeric scaling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "017d7ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed alpha=10.0\n",
      "  Ridge (no numeric scaling):  RMSE = 11290.03   R2 = 0.7921\n",
      "  Ridge (with scaling):       RMSE = 10347.16   R2 = 0.8253\n",
      "\n",
      "After tuning alpha (fair comparison):\n",
      "  No scaling (tuned alpha): best_alpha=0.3511  CV_RMSE=9992.95  Test_RMSE=10334.16  Test_R2=0.8258\n",
      "  Scaled numeric (tuned alpha): best_alpha=0.3511  CV_RMSE=9992.66  Test_RMSE=10291.81  Test_R2=0.8272\n"
     ]
    }
   ],
   "source": [
    "\n",
    "house = pd.read_csv(Path(\"../../../Datasets/Regression/house-prices.csv\"))\n",
    "X = house.drop(columns=[\"Price\"])\n",
    "y = house[\"Price\"].astype(float)\n",
    "\n",
    "numeric_features = [\"SqFt\",\"Bedrooms\",\"Bathrooms\",\"Offers\"]\n",
    "categorical_features = [\"Brick\",\"Neighborhood\"]\n",
    "\n",
    "numeric_no_scale = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\"))\n",
    "])\n",
    "\n",
    "numeric_scaled = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "pre_A = ColumnTransformer([\n",
    "    (\"num\", numeric_no_scale, numeric_features),\n",
    "    (\"cat\", cat_pipe, categorical_features)\n",
    "])\n",
    "\n",
    "pre_B = ColumnTransformer([\n",
    "    (\"num\", numeric_scaled, numeric_features),\n",
    "    (\"cat\", cat_pipe, categorical_features)\n",
    "])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# RMSE helper (compatible across scikit-learn versions)\n",
    "try:\n",
    "    from sklearn.metrics import root_mean_squared_error\n",
    "    def rmse(y_true, y_pred):\n",
    "        return root_mean_squared_error(y_true, y_pred)\n",
    "except Exception:\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    import numpy as np\n",
    "    def rmse(y_true, y_pred):\n",
    "        return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "# (A) Same alpha for both pipelines: shows alpha depends on feature scale\n",
    "alpha_fixed = 10.0\n",
    "pipe_A_fixed = Pipeline([(\"prep\", pre_A), (\"model\", Ridge(alpha=alpha_fixed, random_state=0))])\n",
    "pipe_B_fixed = Pipeline([(\"prep\", pre_B), (\"model\", Ridge(alpha=alpha_fixed, random_state=0))])\n",
    "\n",
    "pipe_A_fixed.fit(X_train, y_train)\n",
    "pipe_B_fixed.fit(X_train, y_train)\n",
    "\n",
    "pred_A = pipe_A_fixed.predict(X_test)\n",
    "pred_B = pipe_B_fixed.predict(X_test)\n",
    "\n",
    "rmse_A = rmse(y_test, pred_A)\n",
    "rmse_B = rmse(y_test, pred_B)\n",
    "r2_A = r2_score(y_test, pred_A)\n",
    "r2_B = r2_score(y_test, pred_B)\n",
    "\n",
    "print(f\"Fixed alpha={alpha_fixed}\")\n",
    "print(\"  Ridge (no numeric scaling):  RMSE =\", round(rmse_A, 2), \"  R2 =\", round(r2_A, 4))\n",
    "print(\"  Ridge (with scaling):       RMSE =\", round(rmse_B, 2), \"  R2 =\", round(r2_B, 4))\n",
    "\n",
    "# (B) Fair comparison: tune alpha separately\n",
    "alphas = np.logspace(-3, 4, 12)\n",
    "\n",
    "def tune_ridge(preprocessor, name):\n",
    "    pipe = Pipeline([(\"prep\", preprocessor), (\"model\", Ridge(random_state=0))])\n",
    "    gs = GridSearchCV(pipe, {\"model__alpha\": alphas}, cv=5, scoring=\"neg_root_mean_squared_error\")\n",
    "    gs.fit(X_train, y_train)\n",
    "    best = gs.best_estimator_\n",
    "    pred = best.predict(X_test)\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"best_alpha\": gs.best_params_[\"model__alpha\"],\n",
    "        \"test_rmse\": rmse(y_test, pred),\n",
    "        \"test_r2\": r2_score(y_test, pred),\n",
    "        \"cv_rmse\": -gs.best_score_,\n",
    "    }\n",
    "\n",
    "res_A = tune_ridge(pre_A, \"No scaling (tuned alpha)\")\n",
    "res_B = tune_ridge(pre_B, \"Scaled numeric (tuned alpha)\")\n",
    "\n",
    "print(\"\\nAfter tuning alpha (fair comparison):\")\n",
    "for r in [res_A, res_B]:\n",
    "    print(f\"  {r['name']}: best_alpha={r['best_alpha']:.4g}  CV_RMSE={r['cv_rmse']:.2f}  Test_RMSE={r['test_rmse']:.2f}  Test_R2={r['test_r2']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c0e177",
   "metadata": {},
   "source": [
    "\n",
    "## 14) Scaler as a hyperparameter: a small grid search (Iris + kNN)\n",
    "\n",
    "At an advanced level, you should *validate the scaler choice* rather than assuming it.\n",
    "\n",
    "Here we compare:\n",
    "\n",
    "- No scaler\n",
    "- Min–max\n",
    "- Standardization\n",
    "- Robust scaling\n",
    "\n",
    "We will do a small `GridSearchCV` where the scaler is part of the pipeline.\n",
    "\n",
    "This is a practical pattern you can reuse for many models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6f00adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV accuracy: 0.9733\n",
      "Best params:\n",
      "  knn__n_neighbors = 11\n",
      "  knn__weights = uniform\n",
      "  scaler = passthrough\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, message=\".*invalid value encountered in cast.*\")\n",
    "\n",
    "\n",
    "X = iris.drop(columns=[\"classification\"])\n",
    "y = iris[\"classification\"]\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"scaler\", \"passthrough\"),\n",
    "    (\"knn\", KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    \"scaler\": [\"passthrough\", StandardScaler(), MinMaxScaler(), RobustScaler()],\n",
    "    \"knn__n_neighbors\": [3,5,7,9,11],\n",
    "    \"knn__weights\": [\"uniform\", \"distance\"]\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "gs = GridSearchCV(pipe, param_grid=param_grid, cv=cv, scoring=\"accuracy\", n_jobs=None)\n",
    "gs.fit(X, y)\n",
    "\n",
    "print(\"Best CV accuracy:\", round(gs.best_score_, 4))\n",
    "print(\"Best params:\")\n",
    "for k, v in gs.best_params_.items():\n",
    "    print(\" \", k, \"=\", v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b59c6d7",
   "metadata": {},
   "source": [
    "\n",
    "## 15) Choosing a scaler: a practitioner’s decision table\n",
    "\n",
    "There is no single best scaler. Choose based on:\n",
    "\n",
    "### Algorithm sensitivity\n",
    "- kNN, k-means, SVM, PCA: scaling is typically essential.\n",
    "- Logistic/linear regression with regularization: scaling is strongly recommended.\n",
    "- Trees: often not required, but can still be part of a consistent pipeline.\n",
    "\n",
    "### Data distribution\n",
    "- Approximately symmetric, not too many outliers → `StandardScaler`\n",
    "- Outliers/heavy tails → `RobustScaler`\n",
    "- Strict bounds needed → `MinMaxScaler`\n",
    "- Sparse features → `MaxAbsScaler` or `StandardScaler(with_mean=False)`\n",
    "- Strong skewness → `PowerTransformer` or `QuantileTransformer` (validate carefully)\n",
    "\n",
    "### Model governance and deployment\n",
    "- Fit scalers only on training data.\n",
    "- Persist preprocessing + model as one pipeline artifact.\n",
    "- Document the scaler choice as part of the model card / experiment record.\n",
    "\n",
    "---\n",
    "\n",
    "## 16) Exercises (recommended)\n",
    "\n",
    "1. On the `diabetes` dataset, compare `StandardScaler` vs `RobustScaler` for `LogisticRegression` using 5-fold CV.\n",
    "2. On `wine`, try an SVM with and without scaling and observe sensitivity to `gamma`.\n",
    "3. On `hw_200`, experiment with `MinMaxScaler` vs `StandardScaler` for k-means and compare cluster centers.\n",
    "4. Implement a small hyperparameter search where the scaler itself is part of the search space.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "Scaling is not cosmetic. It defines the metric structure your model learns from, controls regularization fairness, and can prevent numerical instability. Use pipelines, avoid leakage, and validate scaler choice just like any other modeling hyperparameter.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
